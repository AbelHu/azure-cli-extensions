# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
from typing import Any, Callable, Dict, Generic, List, Optional, TypeVar, Union
import warnings

from azure.core.async_paging import AsyncItemPaged, AsyncList
from azure.core.exceptions import HttpResponseError, ResourceExistsError, ResourceNotFoundError, map_error
from azure.core.pipeline import PipelineResponse
from azure.core.pipeline.transport import AsyncHttpResponse, HttpRequest
from azure.mgmt.core.exceptions import ARMErrorFormat

from ... import models

T = TypeVar('T')
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]

class DatasetOperations:
    """DatasetOperations async operations.

    You should not instantiate this class directly. Instead, you should create a Client instance that
    instantiates it for you and attaches it as an attribute.

    :ivar models: Alias to model classes used in this operation group.
    :type models: ~azure.mgmt.datafactory.models
    :param client: Client for service requests.
    :param config: Configuration of service client.
    :param serializer: An object model serializer.
    :param deserializer: An object model deserializer.
    """

    models = models

    def __init__(self, client, config, serializer, deserializer) -> None:
        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer
        self._config = config

    def list_by_factory(
        self,
        resource_group_name: str,
        factory_name: str,
        **kwargs
    ) -> "models.DatasetListResponse":
        """Lists datasets.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetListResponse or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetListResponse
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetListResponse"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})
        api_version = "2018-06-01"

        def prepare_request(next_link=None):
            if not next_link:
                # Construct URL
                url = self.list_by_factory.metadata['url']
                path_format_arguments = {
                    'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
                    'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
                }
                url = self._client.format_url(url, **path_format_arguments)
            else:
                url = next_link

            # Construct parameters
            query_parameters = {}  # type: Dict[str, Any]
            query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

            # Construct headers
            header_parameters = {}  # type: Dict[str, Any]
            header_parameters['Accept'] = 'application/json'

            # Construct and send request
            request = self._client.get(url, query_parameters, header_parameters)
            return request

        async def extract_data(pipeline_response):
            deserialized = self._deserialize('DatasetListResponse', pipeline_response)
            list_of_elem = deserialized.value
            if cls:
                list_of_elem = cls(list_of_elem)
            return deserialized.next_link or None, AsyncList(list_of_elem)

        async def get_next(next_link=None):
            request = prepare_request(next_link)

            pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
            response = pipeline_response.http_response

            if response.status_code not in [200]:
                map_error(status_code=response.status_code, response=response, error_map=error_map)
                raise HttpResponseError(response=response, error_format=ARMErrorFormat)

            return pipeline_response

        return AsyncItemPaged(
            get_next, extract_data
        )
    list_by_factory.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets'}

    async def create_or_update(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        properties: "models.Dataset",
        if_match: Optional[str] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param properties: Dataset properties.
        :type properties: ~azure.mgmt.datafactory.models.Dataset
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def get(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        if_none_match: Optional[str] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Gets a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param if_none_match: ETag of the dataset entity. Should only be specified for get. If the ETag
         matches the existing entity tag, or if * was provided, then no content will be returned.
        :type if_none_match: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource or None
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})
        api_version = "2018-06-01"

        # Construct URL
        url = self.get.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_none_match is not None:
            header_parameters['If-None-Match'] = self._serialize.header("if_none_match", if_none_match, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        request = self._client.get(url, query_parameters, header_parameters)
        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200, 304]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = None
        if response.status_code == 200:
            deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    get.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def delete(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        **kwargs
    ) -> None:
        """Deletes a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: None or the result of cls(response)
        :rtype: None
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType[None]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})
        api_version = "2018-06-01"

        # Construct URL
        url = self.delete.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]

        # Construct and send request
        request = self._client.delete(url, query_parameters, header_parameters)
        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200, 204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        if cls:
          return cls(pipeline_response, None, {})

    delete.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_amazonmwsobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        amazonmwsobject_type: str,
        amazonmwsobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        amazonmwsobject_description: Optional[str] = None,
        amazonmwsobject_structure: Optional[object] = None,
        amazonmwsobject_schema: Optional[object] = None,
        amazonmwsobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        amazonmwsobject_annotations: Optional[List[object]] = None,
        amazonmwsobject_folder: Optional["models.DatasetFolder"] = None,
        amazonmwsobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param amazonmwsobject_type: Type of dataset.
        :type amazonmwsobject_type: str
        :param amazonmwsobject_linked_service_name: Linked service reference.
        :type amazonmwsobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param amazonmwsobject_description: Dataset description.
        :type amazonmwsobject_description: str
        :param amazonmwsobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type amazonmwsobject_structure: object
        :param amazonmwsobject_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type amazonmwsobject_schema: object
        :param amazonmwsobject_parameters: Parameters for dataset.
        :type amazonmwsobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param amazonmwsobject_annotations: List of tags that can be used for describing the Dataset.
        :type amazonmwsobject_annotations: list[object]
        :param amazonmwsobject_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type amazonmwsobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param amazonmwsobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type amazonmwsobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=amazonmwsobject_type, description=amazonmwsobject_description, structure=amazonmwsobject_structure, schema=amazonmwsobject_schema, linked_service_name=amazonmwsobject_linked_service_name, parameters=amazonmwsobject_parameters, annotations=amazonmwsobject_annotations, folder=amazonmwsobject_folder, table_name=amazonmwsobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_amazonmwsobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_amazonmwsobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_amazonredshifttable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        amazonredshifttable_type: str,
        amazonredshifttable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        amazonredshifttable_description: Optional[str] = None,
        amazonredshifttable_structure: Optional[object] = None,
        amazonredshifttable_schema: Optional[object] = None,
        amazonredshifttable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        amazonredshifttable_annotations: Optional[List[object]] = None,
        amazonredshifttable_folder: Optional["models.DatasetFolder"] = None,
        amazonredshifttable_table_name: Optional[object] = None,
        amazonredshifttable_table: Optional[object] = None,
        amazonredshifttable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param amazonredshifttable_type: Type of dataset.
        :type amazonredshifttable_type: str
        :param amazonredshifttable_linked_service_name: Linked service reference.
        :type amazonredshifttable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param amazonredshifttable_description: Dataset description.
        :type amazonredshifttable_description: str
        :param amazonredshifttable_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type amazonredshifttable_structure: object
        :param amazonredshifttable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type amazonredshifttable_schema: object
        :param amazonredshifttable_parameters: Parameters for dataset.
        :type amazonredshifttable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param amazonredshifttable_annotations: List of tags that can be used for describing the
         Dataset.
        :type amazonredshifttable_annotations: list[object]
        :param amazonredshifttable_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type amazonredshifttable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param amazonredshifttable_table_name: This property will be retired. Please consider using
         schema + table properties instead.
        :type amazonredshifttable_table_name: object
        :param amazonredshifttable_table: The Amazon Redshift table name. Type: string (or Expression
         with resultType string).
        :type amazonredshifttable_table: object
        :param amazonredshifttable_schema_type_properties_schema: The Amazon Redshift schema name.
         Type: string (or Expression with resultType string).
        :type amazonredshifttable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=amazonredshifttable_type, description=amazonredshifttable_description, structure=amazonredshifttable_structure, schema=amazonredshifttable_schema, linked_service_name=amazonredshifttable_linked_service_name, parameters=amazonredshifttable_parameters, annotations=amazonredshifttable_annotations, folder=amazonredshifttable_folder, table_name=amazonredshifttable_table_name, table=amazonredshifttable_table, schema_type_properties_schema=amazonredshifttable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_amazonredshifttable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_amazonredshifttable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_amazons3object(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        amazons3object_type: str,
        amazons3object_linked_service_name: "models.LinkedServiceReference",
        amazons3object_bucket_name: object,
        if_match: Optional[str] = None,
        amazons3object_description: Optional[str] = None,
        amazons3object_structure: Optional[object] = None,
        amazons3object_schema: Optional[object] = None,
        amazons3object_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        amazons3object_annotations: Optional[List[object]] = None,
        amazons3object_folder: Optional["models.DatasetFolder"] = None,
        amazons3object_key: Optional[object] = None,
        amazons3object_prefix: Optional[object] = None,
        amazons3object_version: Optional[object] = None,
        amazons3object_modified_datetime_start: Optional[object] = None,
        amazons3object_modified_datetime_end: Optional[object] = None,
        amazons3object_format: Optional["models.DatasetStorageFormat"] = None,
        amazons3object_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param amazons3object_type: Type of dataset.
        :type amazons3object_type: str
        :param amazons3object_linked_service_name: Linked service reference.
        :type amazons3object_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param amazons3object_bucket_name: The name of the Amazon S3 bucket. Type: string (or
         Expression with resultType string).
        :type amazons3object_bucket_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param amazons3object_description: Dataset description.
        :type amazons3object_description: str
        :param amazons3object_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type amazons3object_structure: object
        :param amazons3object_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type amazons3object_schema: object
        :param amazons3object_parameters: Parameters for dataset.
        :type amazons3object_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param amazons3object_annotations: List of tags that can be used for describing the Dataset.
        :type amazons3object_annotations: list[object]
        :param amazons3object_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type amazons3object_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param amazons3object_key: The key of the Amazon S3 object. Type: string (or Expression with
         resultType string).
        :type amazons3object_key: object
        :param amazons3object_prefix: The prefix filter for the S3 object name. Type: string (or
         Expression with resultType string).
        :type amazons3object_prefix: object
        :param amazons3object_version: The version for the S3 object. Type: string (or Expression with
         resultType string).
        :type amazons3object_version: object
        :param amazons3object_modified_datetime_start: The start of S3 object's modified datetime.
         Type: string (or Expression with resultType string).
        :type amazons3object_modified_datetime_start: object
        :param amazons3object_modified_datetime_end: The end of S3 object's modified datetime. Type:
         string (or Expression with resultType string).
        :type amazons3object_modified_datetime_end: object
        :param amazons3object_format: The format of files.
        :type amazons3object_format: ~azure.mgmt.datafactory.models.DatasetStorageFormat
        :param amazons3object_compression: The data compression method used for the Amazon S3 object.
        :type amazons3object_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=amazons3object_type, description=amazons3object_description, structure=amazons3object_structure, schema=amazons3object_schema, linked_service_name=amazons3object_linked_service_name, parameters=amazons3object_parameters, annotations=amazons3object_annotations, folder=amazons3object_folder, bucket_name=amazons3object_bucket_name, key=amazons3object_key, prefix=amazons3object_prefix, version=amazons3object_version, modified_datetime_start=amazons3object_modified_datetime_start, modified_datetime_end=amazons3object_modified_datetime_end, format=amazons3object_format, compression=amazons3object_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_amazons3object.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_amazons3object.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_avro(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        avro_type: str,
        avro_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        avro_description: Optional[str] = None,
        avro_structure: Optional[object] = None,
        avro_schema: Optional[object] = None,
        avro_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        avro_annotations: Optional[List[object]] = None,
        avro_folder: Optional["models.DatasetFolder"] = None,
        avro_location: Optional["models.DatasetLocation"] = None,
        avro_avro_compression_codec: Optional[Union[str, "models.AvroCompressionCodec"]] = None,
        avro_avro_compression_level: Optional[int] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param avro_type: Type of dataset.
        :type avro_type: str
        :param avro_linked_service_name: Linked service reference.
        :type avro_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param avro_description: Dataset description.
        :type avro_description: str
        :param avro_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type avro_structure: object
        :param avro_schema: Columns that define the physical type schema of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type avro_schema: object
        :param avro_parameters: Parameters for dataset.
        :type avro_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param avro_annotations: List of tags that can be used for describing the Dataset.
        :type avro_annotations: list[object]
        :param avro_folder: The folder that this Dataset is in. If not specified, Dataset will appear
         at the root level.
        :type avro_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param avro_location: The location of the avro storage.
        :type avro_location: ~azure.mgmt.datafactory.models.DatasetLocation
        :param avro_avro_compression_codec:
        :type avro_avro_compression_codec: str or ~azure.mgmt.datafactory.models.AvroCompressionCodec
        :param avro_avro_compression_level:
        :type avro_avro_compression_level: int
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=avro_type, description=avro_description, structure=avro_structure, schema=avro_schema, linked_service_name=avro_linked_service_name, parameters=avro_parameters, annotations=avro_annotations, folder=avro_folder, location=avro_location, avro_compression_codec=avro_avro_compression_codec, avro_compression_level=avro_avro_compression_level)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_avro.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_avro.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azureblob(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azureblob_type: str,
        azureblob_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azureblob_description: Optional[str] = None,
        azureblob_structure: Optional[object] = None,
        azureblob_schema: Optional[object] = None,
        azureblob_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azureblob_annotations: Optional[List[object]] = None,
        azureblob_folder: Optional["models.DatasetFolder"] = None,
        azureblob_folder_path: Optional[object] = None,
        azureblob_table_root_location: Optional[object] = None,
        azureblob_file_name: Optional[object] = None,
        azureblob_modified_datetime_start: Optional[object] = None,
        azureblob_modified_datetime_end: Optional[object] = None,
        azureblob_format: Optional["models.DatasetStorageFormat"] = None,
        azureblob_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azureblob_type: Type of dataset.
        :type azureblob_type: str
        :param azureblob_linked_service_name: Linked service reference.
        :type azureblob_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azureblob_description: Dataset description.
        :type azureblob_description: str
        :param azureblob_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type azureblob_structure: object
        :param azureblob_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azureblob_schema: object
        :param azureblob_parameters: Parameters for dataset.
        :type azureblob_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azureblob_annotations: List of tags that can be used for describing the Dataset.
        :type azureblob_annotations: list[object]
        :param azureblob_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type azureblob_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azureblob_folder_path: The path of the Azure Blob storage. Type: string (or Expression
         with resultType string).
        :type azureblob_folder_path: object
        :param azureblob_table_root_location: The root of blob path. Type: string (or Expression with
         resultType string).
        :type azureblob_table_root_location: object
        :param azureblob_file_name: The name of the Azure Blob. Type: string (or Expression with
         resultType string).
        :type azureblob_file_name: object
        :param azureblob_modified_datetime_start: The start of Azure Blob's modified datetime. Type:
         string (or Expression with resultType string).
        :type azureblob_modified_datetime_start: object
        :param azureblob_modified_datetime_end: The end of Azure Blob's modified datetime. Type: string
         (or Expression with resultType string).
        :type azureblob_modified_datetime_end: object
        :param azureblob_format: The format of the Azure Blob storage.
        :type azureblob_format: ~azure.mgmt.datafactory.models.DatasetStorageFormat
        :param azureblob_compression: The data compression method used for the blob storage.
        :type azureblob_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azureblob_type, description=azureblob_description, structure=azureblob_structure, schema=azureblob_schema, linked_service_name=azureblob_linked_service_name, parameters=azureblob_parameters, annotations=azureblob_annotations, folder=azureblob_folder, folder_path=azureblob_folder_path, table_root_location=azureblob_table_root_location, file_name=azureblob_file_name, modified_datetime_start=azureblob_modified_datetime_start, modified_datetime_end=azureblob_modified_datetime_end, format=azureblob_format, compression=azureblob_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azureblob.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azureblob.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azureblobfsfile(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azureblobfsfile_type: str,
        azureblobfsfile_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azureblobfsfile_description: Optional[str] = None,
        azureblobfsfile_structure: Optional[object] = None,
        azureblobfsfile_schema: Optional[object] = None,
        azureblobfsfile_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azureblobfsfile_annotations: Optional[List[object]] = None,
        azureblobfsfile_folder: Optional["models.DatasetFolder"] = None,
        azureblobfsfile_folder_path: Optional[object] = None,
        azureblobfsfile_file_name: Optional[object] = None,
        azureblobfsfile_format: Optional["models.DatasetStorageFormat"] = None,
        azureblobfsfile_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azureblobfsfile_type: Type of dataset.
        :type azureblobfsfile_type: str
        :param azureblobfsfile_linked_service_name: Linked service reference.
        :type azureblobfsfile_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azureblobfsfile_description: Dataset description.
        :type azureblobfsfile_description: str
        :param azureblobfsfile_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type azureblobfsfile_structure: object
        :param azureblobfsfile_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azureblobfsfile_schema: object
        :param azureblobfsfile_parameters: Parameters for dataset.
        :type azureblobfsfile_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azureblobfsfile_annotations: List of tags that can be used for describing the Dataset.
        :type azureblobfsfile_annotations: list[object]
        :param azureblobfsfile_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type azureblobfsfile_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azureblobfsfile_folder_path: The path of the Azure Data Lake Storage Gen2 storage. Type:
         string (or Expression with resultType string).
        :type azureblobfsfile_folder_path: object
        :param azureblobfsfile_file_name: The name of the Azure Data Lake Storage Gen2. Type: string
         (or Expression with resultType string).
        :type azureblobfsfile_file_name: object
        :param azureblobfsfile_format: The format of the Azure Data Lake Storage Gen2 storage.
        :type azureblobfsfile_format: ~azure.mgmt.datafactory.models.DatasetStorageFormat
        :param azureblobfsfile_compression: The data compression method used for the blob storage.
        :type azureblobfsfile_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azureblobfsfile_type, description=azureblobfsfile_description, structure=azureblobfsfile_structure, schema=azureblobfsfile_schema, linked_service_name=azureblobfsfile_linked_service_name, parameters=azureblobfsfile_parameters, annotations=azureblobfsfile_annotations, folder=azureblobfsfile_folder, folder_path=azureblobfsfile_folder_path, file_name=azureblobfsfile_file_name, format=azureblobfsfile_format, compression=azureblobfsfile_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azureblobfsfile.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azureblobfsfile.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuredataexplorertable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuredataexplorertable_type: str,
        azuredataexplorertable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuredataexplorertable_description: Optional[str] = None,
        azuredataexplorertable_structure: Optional[object] = None,
        azuredataexplorertable_schema: Optional[object] = None,
        azuredataexplorertable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuredataexplorertable_annotations: Optional[List[object]] = None,
        azuredataexplorertable_folder: Optional["models.DatasetFolder"] = None,
        azuredataexplorertable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuredataexplorertable_type: Type of dataset.
        :type azuredataexplorertable_type: str
        :param azuredataexplorertable_linked_service_name: Linked service reference.
        :type azuredataexplorertable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuredataexplorertable_description: Dataset description.
        :type azuredataexplorertable_description: str
        :param azuredataexplorertable_structure: Columns that define the structure of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuredataexplorertable_structure: object
        :param azuredataexplorertable_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuredataexplorertable_schema: object
        :param azuredataexplorertable_parameters: Parameters for dataset.
        :type azuredataexplorertable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuredataexplorertable_annotations: List of tags that can be used for describing the
         Dataset.
        :type azuredataexplorertable_annotations: list[object]
        :param azuredataexplorertable_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type azuredataexplorertable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuredataexplorertable_table: The table name of the Azure Data Explorer database. Type:
         string (or Expression with resultType string).
        :type azuredataexplorertable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuredataexplorertable_type, description=azuredataexplorertable_description, structure=azuredataexplorertable_structure, schema=azuredataexplorertable_schema, linked_service_name=azuredataexplorertable_linked_service_name, parameters=azuredataexplorertable_parameters, annotations=azuredataexplorertable_annotations, folder=azuredataexplorertable_folder, table=azuredataexplorertable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuredataexplorertable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuredataexplorertable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuredatalakestorefile(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuredatalakestorefile_type: str,
        azuredatalakestorefile_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuredatalakestorefile_description: Optional[str] = None,
        azuredatalakestorefile_structure: Optional[object] = None,
        azuredatalakestorefile_schema: Optional[object] = None,
        azuredatalakestorefile_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuredatalakestorefile_annotations: Optional[List[object]] = None,
        azuredatalakestorefile_folder: Optional["models.DatasetFolder"] = None,
        azuredatalakestorefile_folder_path: Optional[object] = None,
        azuredatalakestorefile_file_name: Optional[object] = None,
        azuredatalakestorefile_format: Optional["models.DatasetStorageFormat"] = None,
        azuredatalakestorefile_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuredatalakestorefile_type: Type of dataset.
        :type azuredatalakestorefile_type: str
        :param azuredatalakestorefile_linked_service_name: Linked service reference.
        :type azuredatalakestorefile_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuredatalakestorefile_description: Dataset description.
        :type azuredatalakestorefile_description: str
        :param azuredatalakestorefile_structure: Columns that define the structure of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuredatalakestorefile_structure: object
        :param azuredatalakestorefile_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuredatalakestorefile_schema: object
        :param azuredatalakestorefile_parameters: Parameters for dataset.
        :type azuredatalakestorefile_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuredatalakestorefile_annotations: List of tags that can be used for describing the
         Dataset.
        :type azuredatalakestorefile_annotations: list[object]
        :param azuredatalakestorefile_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type azuredatalakestorefile_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuredatalakestorefile_folder_path: Path to the folder in the Azure Data Lake Store.
         Type: string (or Expression with resultType string).
        :type azuredatalakestorefile_folder_path: object
        :param azuredatalakestorefile_file_name: The name of the file in the Azure Data Lake Store.
         Type: string (or Expression with resultType string).
        :type azuredatalakestorefile_file_name: object
        :param azuredatalakestorefile_format: The format of the Data Lake Store.
        :type azuredatalakestorefile_format: ~azure.mgmt.datafactory.models.DatasetStorageFormat
        :param azuredatalakestorefile_compression: The data compression method used for the item(s) in
         the Azure Data Lake Store.
        :type azuredatalakestorefile_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuredatalakestorefile_type, description=azuredatalakestorefile_description, structure=azuredatalakestorefile_structure, schema=azuredatalakestorefile_schema, linked_service_name=azuredatalakestorefile_linked_service_name, parameters=azuredatalakestorefile_parameters, annotations=azuredatalakestorefile_annotations, folder=azuredatalakestorefile_folder, folder_path=azuredatalakestorefile_folder_path, file_name=azuredatalakestorefile_file_name, format=azuredatalakestorefile_format, compression=azuredatalakestorefile_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuredatalakestorefile.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuredatalakestorefile.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuremariadbtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuremariadbtable_type: str,
        azuremariadbtable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuremariadbtable_description: Optional[str] = None,
        azuremariadbtable_structure: Optional[object] = None,
        azuremariadbtable_schema: Optional[object] = None,
        azuremariadbtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuremariadbtable_annotations: Optional[List[object]] = None,
        azuremariadbtable_folder: Optional["models.DatasetFolder"] = None,
        azuremariadbtable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuremariadbtable_type: Type of dataset.
        :type azuremariadbtable_type: str
        :param azuremariadbtable_linked_service_name: Linked service reference.
        :type azuremariadbtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuremariadbtable_description: Dataset description.
        :type azuremariadbtable_description: str
        :param azuremariadbtable_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuremariadbtable_structure: object
        :param azuremariadbtable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuremariadbtable_schema: object
        :param azuremariadbtable_parameters: Parameters for dataset.
        :type azuremariadbtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuremariadbtable_annotations: List of tags that can be used for describing the Dataset.
        :type azuremariadbtable_annotations: list[object]
        :param azuremariadbtable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type azuremariadbtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuremariadbtable_table_name: The table name. Type: string (or Expression with
         resultType string).
        :type azuremariadbtable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuremariadbtable_type, description=azuremariadbtable_description, structure=azuremariadbtable_structure, schema=azuremariadbtable_schema, linked_service_name=azuremariadbtable_linked_service_name, parameters=azuremariadbtable_parameters, annotations=azuremariadbtable_annotations, folder=azuremariadbtable_folder, table_name=azuremariadbtable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuremariadbtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuremariadbtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuremysqltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuremysqltable_type: str,
        azuremysqltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuremysqltable_description: Optional[str] = None,
        azuremysqltable_structure: Optional[object] = None,
        azuremysqltable_schema: Optional[object] = None,
        azuremysqltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuremysqltable_annotations: Optional[List[object]] = None,
        azuremysqltable_folder: Optional["models.DatasetFolder"] = None,
        azuremysqltable_table_name: Optional[object] = None,
        azuremysqltable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuremysqltable_type: Type of dataset.
        :type azuremysqltable_type: str
        :param azuremysqltable_linked_service_name: Linked service reference.
        :type azuremysqltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuremysqltable_description: Dataset description.
        :type azuremysqltable_description: str
        :param azuremysqltable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuremysqltable_structure: object
        :param azuremysqltable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuremysqltable_schema: object
        :param azuremysqltable_parameters: Parameters for dataset.
        :type azuremysqltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuremysqltable_annotations: List of tags that can be used for describing the Dataset.
        :type azuremysqltable_annotations: list[object]
        :param azuremysqltable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type azuremysqltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuremysqltable_table_name: The Azure MySQL database table name. Type: string (or
         Expression with resultType string).
        :type azuremysqltable_table_name: object
        :param azuremysqltable_table: The name of Azure MySQL database table. Type: string (or
         Expression with resultType string).
        :type azuremysqltable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuremysqltable_type, description=azuremysqltable_description, structure=azuremysqltable_structure, schema=azuremysqltable_schema, linked_service_name=azuremysqltable_linked_service_name, parameters=azuremysqltable_parameters, annotations=azuremysqltable_annotations, folder=azuremysqltable_folder, table_name=azuremysqltable_table_name, table=azuremysqltable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuremysqltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuremysqltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azurepostgresqltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azurepostgresqltable_type: str,
        azurepostgresqltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azurepostgresqltable_description: Optional[str] = None,
        azurepostgresqltable_structure: Optional[object] = None,
        azurepostgresqltable_schema: Optional[object] = None,
        azurepostgresqltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azurepostgresqltable_annotations: Optional[List[object]] = None,
        azurepostgresqltable_folder: Optional["models.DatasetFolder"] = None,
        azurepostgresqltable_table_name: Optional[object] = None,
        azurepostgresqltable_table: Optional[object] = None,
        azurepostgresqltable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azurepostgresqltable_type: Type of dataset.
        :type azurepostgresqltable_type: str
        :param azurepostgresqltable_linked_service_name: Linked service reference.
        :type azurepostgresqltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azurepostgresqltable_description: Dataset description.
        :type azurepostgresqltable_description: str
        :param azurepostgresqltable_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type azurepostgresqltable_structure: object
        :param azurepostgresqltable_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azurepostgresqltable_schema: object
        :param azurepostgresqltable_parameters: Parameters for dataset.
        :type azurepostgresqltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azurepostgresqltable_annotations: List of tags that can be used for describing the
         Dataset.
        :type azurepostgresqltable_annotations: list[object]
        :param azurepostgresqltable_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type azurepostgresqltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azurepostgresqltable_table_name: The table name of the Azure PostgreSQL database which
         includes both schema and table. Type: string (or Expression with resultType string).
        :type azurepostgresqltable_table_name: object
        :param azurepostgresqltable_table: The table name of the Azure PostgreSQL database. Type:
         string (or Expression with resultType string).
        :type azurepostgresqltable_table: object
        :param azurepostgresqltable_schema_type_properties_schema: The schema name of the Azure
         PostgreSQL database. Type: string (or Expression with resultType string).
        :type azurepostgresqltable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azurepostgresqltable_type, description=azurepostgresqltable_description, structure=azurepostgresqltable_structure, schema=azurepostgresqltable_schema, linked_service_name=azurepostgresqltable_linked_service_name, parameters=azurepostgresqltable_parameters, annotations=azurepostgresqltable_annotations, folder=azurepostgresqltable_folder, table_name=azurepostgresqltable_table_name, table=azurepostgresqltable_table, schema_type_properties_schema=azurepostgresqltable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azurepostgresqltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azurepostgresqltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuresearchindex(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuresearchindex_type: str,
        azuresearchindex_linked_service_name: "models.LinkedServiceReference",
        azuresearchindex_index_name: object,
        if_match: Optional[str] = None,
        azuresearchindex_description: Optional[str] = None,
        azuresearchindex_structure: Optional[object] = None,
        azuresearchindex_schema: Optional[object] = None,
        azuresearchindex_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuresearchindex_annotations: Optional[List[object]] = None,
        azuresearchindex_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuresearchindex_type: Type of dataset.
        :type azuresearchindex_type: str
        :param azuresearchindex_linked_service_name: Linked service reference.
        :type azuresearchindex_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param azuresearchindex_index_name: The name of the Azure Search Index. Type: string (or
         Expression with resultType string).
        :type azuresearchindex_index_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuresearchindex_description: Dataset description.
        :type azuresearchindex_description: str
        :param azuresearchindex_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuresearchindex_structure: object
        :param azuresearchindex_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuresearchindex_schema: object
        :param azuresearchindex_parameters: Parameters for dataset.
        :type azuresearchindex_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuresearchindex_annotations: List of tags that can be used for describing the Dataset.
        :type azuresearchindex_annotations: list[object]
        :param azuresearchindex_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type azuresearchindex_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuresearchindex_type, description=azuresearchindex_description, structure=azuresearchindex_structure, schema=azuresearchindex_schema, linked_service_name=azuresearchindex_linked_service_name, parameters=azuresearchindex_parameters, annotations=azuresearchindex_annotations, folder=azuresearchindex_folder, index_name=azuresearchindex_index_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuresearchindex.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuresearchindex.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuresqldwtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuresqldwtable_type: str,
        azuresqldwtable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuresqldwtable_description: Optional[str] = None,
        azuresqldwtable_structure: Optional[object] = None,
        azuresqldwtable_schema: Optional[object] = None,
        azuresqldwtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuresqldwtable_annotations: Optional[List[object]] = None,
        azuresqldwtable_folder: Optional["models.DatasetFolder"] = None,
        azuresqldwtable_table_name: Optional[object] = None,
        azuresqldwtable_schema_type_properties_schema: Optional[object] = None,
        azuresqldwtable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuresqldwtable_type: Type of dataset.
        :type azuresqldwtable_type: str
        :param azuresqldwtable_linked_service_name: Linked service reference.
        :type azuresqldwtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuresqldwtable_description: Dataset description.
        :type azuresqldwtable_description: str
        :param azuresqldwtable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuresqldwtable_structure: object
        :param azuresqldwtable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuresqldwtable_schema: object
        :param azuresqldwtable_parameters: Parameters for dataset.
        :type azuresqldwtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuresqldwtable_annotations: List of tags that can be used for describing the Dataset.
        :type azuresqldwtable_annotations: list[object]
        :param azuresqldwtable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type azuresqldwtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuresqldwtable_table_name: This property will be retired. Please consider using schema
         + table properties instead.
        :type azuresqldwtable_table_name: object
        :param azuresqldwtable_schema_type_properties_schema: The schema name of the Azure SQL Data
         Warehouse. Type: string (or Expression with resultType string).
        :type azuresqldwtable_schema_type_properties_schema: object
        :param azuresqldwtable_table: The table name of the Azure SQL Data Warehouse. Type: string (or
         Expression with resultType string).
        :type azuresqldwtable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuresqldwtable_type, description=azuresqldwtable_description, structure=azuresqldwtable_structure, schema=azuresqldwtable_schema, linked_service_name=azuresqldwtable_linked_service_name, parameters=azuresqldwtable_parameters, annotations=azuresqldwtable_annotations, folder=azuresqldwtable_folder, table_name=azuresqldwtable_table_name, schema_type_properties_schema=azuresqldwtable_schema_type_properties_schema, table=azuresqldwtable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuresqldwtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuresqldwtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuresqlmitable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuresqlmitable_type: str,
        azuresqlmitable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuresqlmitable_description: Optional[str] = None,
        azuresqlmitable_structure: Optional[object] = None,
        azuresqlmitable_schema: Optional[object] = None,
        azuresqlmitable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuresqlmitable_annotations: Optional[List[object]] = None,
        azuresqlmitable_folder: Optional["models.DatasetFolder"] = None,
        azuresqlmitable_table_name: Optional[object] = None,
        azuresqlmitable_schema_type_properties_schema: Optional[object] = None,
        azuresqlmitable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuresqlmitable_type: Type of dataset.
        :type azuresqlmitable_type: str
        :param azuresqlmitable_linked_service_name: Linked service reference.
        :type azuresqlmitable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuresqlmitable_description: Dataset description.
        :type azuresqlmitable_description: str
        :param azuresqlmitable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuresqlmitable_structure: object
        :param azuresqlmitable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuresqlmitable_schema: object
        :param azuresqlmitable_parameters: Parameters for dataset.
        :type azuresqlmitable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuresqlmitable_annotations: List of tags that can be used for describing the Dataset.
        :type azuresqlmitable_annotations: list[object]
        :param azuresqlmitable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type azuresqlmitable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuresqlmitable_table_name: This property will be retired. Please consider using schema
         + table properties instead.
        :type azuresqlmitable_table_name: object
        :param azuresqlmitable_schema_type_properties_schema: The schema name of the Azure SQL Managed
         Instance. Type: string (or Expression with resultType string).
        :type azuresqlmitable_schema_type_properties_schema: object
        :param azuresqlmitable_table: The table name of the Azure SQL Managed Instance dataset. Type:
         string (or Expression with resultType string).
        :type azuresqlmitable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuresqlmitable_type, description=azuresqlmitable_description, structure=azuresqlmitable_structure, schema=azuresqlmitable_schema, linked_service_name=azuresqlmitable_linked_service_name, parameters=azuresqlmitable_parameters, annotations=azuresqlmitable_annotations, folder=azuresqlmitable_folder, table_name=azuresqlmitable_table_name, schema_type_properties_schema=azuresqlmitable_schema_type_properties_schema, table=azuresqlmitable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuresqlmitable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuresqlmitable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuresqltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuresqltable_type: str,
        azuresqltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        azuresqltable_description: Optional[str] = None,
        azuresqltable_structure: Optional[object] = None,
        azuresqltable_schema: Optional[object] = None,
        azuresqltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuresqltable_annotations: Optional[List[object]] = None,
        azuresqltable_folder: Optional["models.DatasetFolder"] = None,
        azuresqltable_table_name: Optional[object] = None,
        azuresqltable_schema_type_properties_schema: Optional[object] = None,
        azuresqltable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuresqltable_type: Type of dataset.
        :type azuresqltable_type: str
        :param azuresqltable_linked_service_name: Linked service reference.
        :type azuresqltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuresqltable_description: Dataset description.
        :type azuresqltable_description: str
        :param azuresqltable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type azuresqltable_structure: object
        :param azuresqltable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuresqltable_schema: object
        :param azuresqltable_parameters: Parameters for dataset.
        :type azuresqltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuresqltable_annotations: List of tags that can be used for describing the Dataset.
        :type azuresqltable_annotations: list[object]
        :param azuresqltable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type azuresqltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param azuresqltable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type azuresqltable_table_name: object
        :param azuresqltable_schema_type_properties_schema: The schema name of the Azure SQL database.
         Type: string (or Expression with resultType string).
        :type azuresqltable_schema_type_properties_schema: object
        :param azuresqltable_table: The table name of the Azure SQL database. Type: string (or
         Expression with resultType string).
        :type azuresqltable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuresqltable_type, description=azuresqltable_description, structure=azuresqltable_structure, schema=azuresqltable_schema, linked_service_name=azuresqltable_linked_service_name, parameters=azuresqltable_parameters, annotations=azuresqltable_annotations, folder=azuresqltable_folder, table_name=azuresqltable_table_name, schema_type_properties_schema=azuresqltable_schema_type_properties_schema, table=azuresqltable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuresqltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuresqltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_azuretable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        azuretable_type: str,
        azuretable_linked_service_name: "models.LinkedServiceReference",
        azuretable_table_name: object,
        if_match: Optional[str] = None,
        azuretable_description: Optional[str] = None,
        azuretable_structure: Optional[object] = None,
        azuretable_schema: Optional[object] = None,
        azuretable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        azuretable_annotations: Optional[List[object]] = None,
        azuretable_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param azuretable_type: Type of dataset.
        :type azuretable_type: str
        :param azuretable_linked_service_name: Linked service reference.
        :type azuretable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param azuretable_table_name: The table name of the Azure Table storage. Type: string (or
         Expression with resultType string).
        :type azuretable_table_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param azuretable_description: Dataset description.
        :type azuretable_description: str
        :param azuretable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type azuretable_structure: object
        :param azuretable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type azuretable_schema: object
        :param azuretable_parameters: Parameters for dataset.
        :type azuretable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param azuretable_annotations: List of tags that can be used for describing the Dataset.
        :type azuretable_annotations: list[object]
        :param azuretable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type azuretable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=azuretable_type, description=azuretable_description, structure=azuretable_structure, schema=azuretable_schema, linked_service_name=azuretable_linked_service_name, parameters=azuretable_parameters, annotations=azuretable_annotations, folder=azuretable_folder, table_name=azuretable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_azuretable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_azuretable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_binary(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        binary_type: str,
        binary_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        binary_description: Optional[str] = None,
        binary_structure: Optional[object] = None,
        binary_schema: Optional[object] = None,
        binary_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        binary_annotations: Optional[List[object]] = None,
        binary_folder: Optional["models.DatasetFolder"] = None,
        binary_location: Optional["models.DatasetLocation"] = None,
        binary_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param binary_type: Type of dataset.
        :type binary_type: str
        :param binary_linked_service_name: Linked service reference.
        :type binary_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param binary_description: Dataset description.
        :type binary_description: str
        :param binary_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type binary_structure: object
        :param binary_schema: Columns that define the physical type schema of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type binary_schema: object
        :param binary_parameters: Parameters for dataset.
        :type binary_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param binary_annotations: List of tags that can be used for describing the Dataset.
        :type binary_annotations: list[object]
        :param binary_folder: The folder that this Dataset is in. If not specified, Dataset will appear
         at the root level.
        :type binary_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param binary_location: The location of the Binary storage.
        :type binary_location: ~azure.mgmt.datafactory.models.DatasetLocation
        :param binary_compression: The data compression method used for the binary dataset.
        :type binary_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=binary_type, description=binary_description, structure=binary_structure, schema=binary_schema, linked_service_name=binary_linked_service_name, parameters=binary_parameters, annotations=binary_annotations, folder=binary_folder, location=binary_location, compression=binary_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_binary.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_binary.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_cassandratable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        cassandratable_type: str,
        cassandratable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        cassandratable_description: Optional[str] = None,
        cassandratable_structure: Optional[object] = None,
        cassandratable_schema: Optional[object] = None,
        cassandratable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        cassandratable_annotations: Optional[List[object]] = None,
        cassandratable_folder: Optional["models.DatasetFolder"] = None,
        cassandratable_table_name: Optional[object] = None,
        cassandratable_keyspace: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param cassandratable_type: Type of dataset.
        :type cassandratable_type: str
        :param cassandratable_linked_service_name: Linked service reference.
        :type cassandratable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param cassandratable_description: Dataset description.
        :type cassandratable_description: str
        :param cassandratable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type cassandratable_structure: object
        :param cassandratable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type cassandratable_schema: object
        :param cassandratable_parameters: Parameters for dataset.
        :type cassandratable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param cassandratable_annotations: List of tags that can be used for describing the Dataset.
        :type cassandratable_annotations: list[object]
        :param cassandratable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type cassandratable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param cassandratable_table_name: The table name of the Cassandra database. Type: string (or
         Expression with resultType string).
        :type cassandratable_table_name: object
        :param cassandratable_keyspace: The keyspace of the Cassandra database. Type: string (or
         Expression with resultType string).
        :type cassandratable_keyspace: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=cassandratable_type, description=cassandratable_description, structure=cassandratable_structure, schema=cassandratable_schema, linked_service_name=cassandratable_linked_service_name, parameters=cassandratable_parameters, annotations=cassandratable_annotations, folder=cassandratable_folder, table_name=cassandratable_table_name, keyspace=cassandratable_keyspace)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_cassandratable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_cassandratable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_commondataserviceforappsentity(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        commondataserviceforappsentity_type: str,
        commondataserviceforappsentity_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        commondataserviceforappsentity_description: Optional[str] = None,
        commondataserviceforappsentity_structure: Optional[object] = None,
        commondataserviceforappsentity_schema: Optional[object] = None,
        commondataserviceforappsentity_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        commondataserviceforappsentity_annotations: Optional[List[object]] = None,
        commondataserviceforappsentity_folder: Optional["models.DatasetFolder"] = None,
        commondataserviceforappsentity_entity_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param commondataserviceforappsentity_type: Type of dataset.
        :type commondataserviceforappsentity_type: str
        :param commondataserviceforappsentity_linked_service_name: Linked service reference.
        :type commondataserviceforappsentity_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param commondataserviceforappsentity_description: Dataset description.
        :type commondataserviceforappsentity_description: str
        :param commondataserviceforappsentity_structure: Columns that define the structure of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type commondataserviceforappsentity_structure: object
        :param commondataserviceforappsentity_schema: Columns that define the physical type schema of
         the dataset. Type: array (or Expression with resultType array), itemType:
         DatasetSchemaDataElement.
        :type commondataserviceforappsentity_schema: object
        :param commondataserviceforappsentity_parameters: Parameters for dataset.
        :type commondataserviceforappsentity_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param commondataserviceforappsentity_annotations: List of tags that can be used for describing
         the Dataset.
        :type commondataserviceforappsentity_annotations: list[object]
        :param commondataserviceforappsentity_folder: The folder that this Dataset is in. If not
         specified, Dataset will appear at the root level.
        :type commondataserviceforappsentity_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param commondataserviceforappsentity_entity_name: The logical name of the entity. Type: string
         (or Expression with resultType string).
        :type commondataserviceforappsentity_entity_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=commondataserviceforappsentity_type, description=commondataserviceforappsentity_description, structure=commondataserviceforappsentity_structure, schema=commondataserviceforappsentity_schema, linked_service_name=commondataserviceforappsentity_linked_service_name, parameters=commondataserviceforappsentity_parameters, annotations=commondataserviceforappsentity_annotations, folder=commondataserviceforappsentity_folder, entity_name=commondataserviceforappsentity_entity_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_commondataserviceforappsentity.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_commondataserviceforappsentity.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_concurobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        concurobject_type: str,
        concurobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        concurobject_description: Optional[str] = None,
        concurobject_structure: Optional[object] = None,
        concurobject_schema: Optional[object] = None,
        concurobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        concurobject_annotations: Optional[List[object]] = None,
        concurobject_folder: Optional["models.DatasetFolder"] = None,
        concurobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param concurobject_type: Type of dataset.
        :type concurobject_type: str
        :param concurobject_linked_service_name: Linked service reference.
        :type concurobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param concurobject_description: Dataset description.
        :type concurobject_description: str
        :param concurobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type concurobject_structure: object
        :param concurobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type concurobject_schema: object
        :param concurobject_parameters: Parameters for dataset.
        :type concurobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param concurobject_annotations: List of tags that can be used for describing the Dataset.
        :type concurobject_annotations: list[object]
        :param concurobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type concurobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param concurobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type concurobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=concurobject_type, description=concurobject_description, structure=concurobject_structure, schema=concurobject_schema, linked_service_name=concurobject_linked_service_name, parameters=concurobject_parameters, annotations=concurobject_annotations, folder=concurobject_folder, table_name=concurobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_concurobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_concurobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_cosmosdbmongodbapicollection(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        cosmosdbmongodbapicollection_type: str,
        cosmosdbmongodbapicollection_linked_service_name: "models.LinkedServiceReference",
        cosmosdbmongodbapicollection_collection: object,
        if_match: Optional[str] = None,
        cosmosdbmongodbapicollection_description: Optional[str] = None,
        cosmosdbmongodbapicollection_structure: Optional[object] = None,
        cosmosdbmongodbapicollection_schema: Optional[object] = None,
        cosmosdbmongodbapicollection_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        cosmosdbmongodbapicollection_annotations: Optional[List[object]] = None,
        cosmosdbmongodbapicollection_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param cosmosdbmongodbapicollection_type: Type of dataset.
        :type cosmosdbmongodbapicollection_type: str
        :param cosmosdbmongodbapicollection_linked_service_name: Linked service reference.
        :type cosmosdbmongodbapicollection_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param cosmosdbmongodbapicollection_collection: The collection name of the CosmosDB (MongoDB
         API) database. Type: string (or Expression with resultType string).
        :type cosmosdbmongodbapicollection_collection: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param cosmosdbmongodbapicollection_description: Dataset description.
        :type cosmosdbmongodbapicollection_description: str
        :param cosmosdbmongodbapicollection_structure: Columns that define the structure of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type cosmosdbmongodbapicollection_structure: object
        :param cosmosdbmongodbapicollection_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type cosmosdbmongodbapicollection_schema: object
        :param cosmosdbmongodbapicollection_parameters: Parameters for dataset.
        :type cosmosdbmongodbapicollection_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param cosmosdbmongodbapicollection_annotations: List of tags that can be used for describing
         the Dataset.
        :type cosmosdbmongodbapicollection_annotations: list[object]
        :param cosmosdbmongodbapicollection_folder: The folder that this Dataset is in. If not
         specified, Dataset will appear at the root level.
        :type cosmosdbmongodbapicollection_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=cosmosdbmongodbapicollection_type, description=cosmosdbmongodbapicollection_description, structure=cosmosdbmongodbapicollection_structure, schema=cosmosdbmongodbapicollection_schema, linked_service_name=cosmosdbmongodbapicollection_linked_service_name, parameters=cosmosdbmongodbapicollection_parameters, annotations=cosmosdbmongodbapicollection_annotations, folder=cosmosdbmongodbapicollection_folder, collection=cosmosdbmongodbapicollection_collection)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_cosmosdbmongodbapicollection.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_cosmosdbmongodbapicollection.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_cosmosdbsqlapicollection(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        properties: "models.CosmosDBSqlApiCollectionDataset",
        if_match: Optional[str] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param properties: Dataset properties.
        :type properties: ~azure.mgmt.datafactory.models.CosmosDBSqlApiCollectionDataset
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_cosmosdbsqlapicollection.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_cosmosdbsqlapicollection.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_couchbasetable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        couchbasetable_type: str,
        couchbasetable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        couchbasetable_description: Optional[str] = None,
        couchbasetable_structure: Optional[object] = None,
        couchbasetable_schema: Optional[object] = None,
        couchbasetable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        couchbasetable_annotations: Optional[List[object]] = None,
        couchbasetable_folder: Optional["models.DatasetFolder"] = None,
        couchbasetable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param couchbasetable_type: Type of dataset.
        :type couchbasetable_type: str
        :param couchbasetable_linked_service_name: Linked service reference.
        :type couchbasetable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param couchbasetable_description: Dataset description.
        :type couchbasetable_description: str
        :param couchbasetable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type couchbasetable_structure: object
        :param couchbasetable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type couchbasetable_schema: object
        :param couchbasetable_parameters: Parameters for dataset.
        :type couchbasetable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param couchbasetable_annotations: List of tags that can be used for describing the Dataset.
        :type couchbasetable_annotations: list[object]
        :param couchbasetable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type couchbasetable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param couchbasetable_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type couchbasetable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=couchbasetable_type, description=couchbasetable_description, structure=couchbasetable_structure, schema=couchbasetable_schema, linked_service_name=couchbasetable_linked_service_name, parameters=couchbasetable_parameters, annotations=couchbasetable_annotations, folder=couchbasetable_folder, table_name=couchbasetable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_couchbasetable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_couchbasetable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_customdataset(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        customdataset_type: str,
        customdataset_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        customdataset_description: Optional[str] = None,
        customdataset_structure: Optional[object] = None,
        customdataset_schema: Optional[object] = None,
        customdataset_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        customdataset_annotations: Optional[List[object]] = None,
        customdataset_folder: Optional["models.DatasetFolder"] = None,
        customdataset_type_properties: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param customdataset_type: Type of dataset.
        :type customdataset_type: str
        :param customdataset_linked_service_name: Linked service reference.
        :type customdataset_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param customdataset_description: Dataset description.
        :type customdataset_description: str
        :param customdataset_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type customdataset_structure: object
        :param customdataset_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type customdataset_schema: object
        :param customdataset_parameters: Parameters for dataset.
        :type customdataset_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param customdataset_annotations: List of tags that can be used for describing the Dataset.
        :type customdataset_annotations: list[object]
        :param customdataset_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type customdataset_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param customdataset_type_properties: Custom dataset properties.
        :type customdataset_type_properties: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=customdataset_type, description=customdataset_description, structure=customdataset_structure, schema=customdataset_schema, linked_service_name=customdataset_linked_service_name, parameters=customdataset_parameters, annotations=customdataset_annotations, folder=customdataset_folder, type_properties=customdataset_type_properties)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_customdataset.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_customdataset.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_db2table(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        db2table_type: str,
        db2table_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        db2table_description: Optional[str] = None,
        db2table_structure: Optional[object] = None,
        db2table_schema: Optional[object] = None,
        db2table_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        db2table_annotations: Optional[List[object]] = None,
        db2table_folder: Optional["models.DatasetFolder"] = None,
        db2table_table_name: Optional[object] = None,
        db2table_schema_type_properties_schema: Optional[object] = None,
        db2table_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param db2table_type: Type of dataset.
        :type db2table_type: str
        :param db2table_linked_service_name: Linked service reference.
        :type db2table_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param db2table_description: Dataset description.
        :type db2table_description: str
        :param db2table_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type db2table_structure: object
        :param db2table_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type db2table_schema: object
        :param db2table_parameters: Parameters for dataset.
        :type db2table_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param db2table_annotations: List of tags that can be used for describing the Dataset.
        :type db2table_annotations: list[object]
        :param db2table_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type db2table_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param db2table_table_name: This property will be retired. Please consider using schema + table
         properties instead.
        :type db2table_table_name: object
        :param db2table_schema_type_properties_schema: The Db2 schema name. Type: string (or Expression
         with resultType string).
        :type db2table_schema_type_properties_schema: object
        :param db2table_table: The Db2 table name. Type: string (or Expression with resultType string).
        :type db2table_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=db2table_type, description=db2table_description, structure=db2table_structure, schema=db2table_schema, linked_service_name=db2table_linked_service_name, parameters=db2table_parameters, annotations=db2table_annotations, folder=db2table_folder, table_name=db2table_table_name, schema_type_properties_schema=db2table_schema_type_properties_schema, table=db2table_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_db2table.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_db2table.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_delimitedtext(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        delimitedtext_type: str,
        delimitedtext_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        delimitedtext_description: Optional[str] = None,
        delimitedtext_structure: Optional[object] = None,
        delimitedtext_schema: Optional[object] = None,
        delimitedtext_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        delimitedtext_annotations: Optional[List[object]] = None,
        delimitedtext_folder: Optional["models.DatasetFolder"] = None,
        delimitedtext_location: Optional["models.DatasetLocation"] = None,
        delimitedtext_column_delimiter: Optional[object] = None,
        delimitedtext_row_delimiter: Optional[object] = None,
        delimitedtext_encoding_name: Optional[object] = None,
        delimitedtext_compression_codec: Optional[Union[str, "models.CompressionCodec"]] = None,
        delimitedtext_compression_level: Optional[Union[str, "models.DatasetCompressionLevel"]] = None,
        delimitedtext_quote_char: Optional[object] = None,
        delimitedtext_escape_char: Optional[object] = None,
        delimitedtext_first_row_as_header: Optional[object] = None,
        delimitedtext_null_value: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param delimitedtext_type: Type of dataset.
        :type delimitedtext_type: str
        :param delimitedtext_linked_service_name: Linked service reference.
        :type delimitedtext_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param delimitedtext_description: Dataset description.
        :type delimitedtext_description: str
        :param delimitedtext_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type delimitedtext_structure: object
        :param delimitedtext_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type delimitedtext_schema: object
        :param delimitedtext_parameters: Parameters for dataset.
        :type delimitedtext_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param delimitedtext_annotations: List of tags that can be used for describing the Dataset.
        :type delimitedtext_annotations: list[object]
        :param delimitedtext_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type delimitedtext_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param delimitedtext_location: The location of the delimited text storage.
        :type delimitedtext_location: ~azure.mgmt.datafactory.models.DatasetLocation
        :param delimitedtext_column_delimiter: The column delimiter. Type: string (or Expression with
         resultType string).
        :type delimitedtext_column_delimiter: object
        :param delimitedtext_row_delimiter: The row delimiter. Type: string (or Expression with
         resultType string).
        :type delimitedtext_row_delimiter: object
        :param delimitedtext_encoding_name: The code page name of the preferred encoding. If miss, the
         default value is UTF-8, unless BOM denotes another Unicode encoding. Refer to the name column
         of the table in the following link to set supported values:
         https://msdn.microsoft.com/library/system.text.encoding.aspx. Type: string (or Expression with
         resultType string).
        :type delimitedtext_encoding_name: object
        :param delimitedtext_compression_codec:
        :type delimitedtext_compression_codec: str or ~azure.mgmt.datafactory.models.CompressionCodec
        :param delimitedtext_compression_level: The data compression method used for DelimitedText.
        :type delimitedtext_compression_level: str or ~azure.mgmt.datafactory.models.DatasetCompressionLevel
        :param delimitedtext_quote_char: The quote character. Type: string (or Expression with
         resultType string).
        :type delimitedtext_quote_char: object
        :param delimitedtext_escape_char: The escape character. Type: string (or Expression with
         resultType string).
        :type delimitedtext_escape_char: object
        :param delimitedtext_first_row_as_header: When used as input, treat the first row of data as
         headers. When used as output,write the headers into the output as the first row of data. The
         default value is false. Type: boolean (or Expression with resultType boolean).
        :type delimitedtext_first_row_as_header: object
        :param delimitedtext_null_value: The null value string. Type: string (or Expression with
         resultType string).
        :type delimitedtext_null_value: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=delimitedtext_type, description=delimitedtext_description, structure=delimitedtext_structure, schema=delimitedtext_schema, linked_service_name=delimitedtext_linked_service_name, parameters=delimitedtext_parameters, annotations=delimitedtext_annotations, folder=delimitedtext_folder, location=delimitedtext_location, column_delimiter=delimitedtext_column_delimiter, row_delimiter=delimitedtext_row_delimiter, encoding_name=delimitedtext_encoding_name, compression_codec=delimitedtext_compression_codec, compression_level=delimitedtext_compression_level, quote_char=delimitedtext_quote_char, escape_char=delimitedtext_escape_char, first_row_as_header=delimitedtext_first_row_as_header, null_value=delimitedtext_null_value)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_delimitedtext.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_delimitedtext.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_documentdbcollection(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        documentdbcollection_type: str,
        documentdbcollection_linked_service_name: "models.LinkedServiceReference",
        documentdbcollection_collection_name: object,
        if_match: Optional[str] = None,
        documentdbcollection_description: Optional[str] = None,
        documentdbcollection_structure: Optional[object] = None,
        documentdbcollection_schema: Optional[object] = None,
        documentdbcollection_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        documentdbcollection_annotations: Optional[List[object]] = None,
        documentdbcollection_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param documentdbcollection_type: Type of dataset.
        :type documentdbcollection_type: str
        :param documentdbcollection_linked_service_name: Linked service reference.
        :type documentdbcollection_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param documentdbcollection_collection_name: Document Database collection name. Type: string
         (or Expression with resultType string).
        :type documentdbcollection_collection_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param documentdbcollection_description: Dataset description.
        :type documentdbcollection_description: str
        :param documentdbcollection_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type documentdbcollection_structure: object
        :param documentdbcollection_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type documentdbcollection_schema: object
        :param documentdbcollection_parameters: Parameters for dataset.
        :type documentdbcollection_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param documentdbcollection_annotations: List of tags that can be used for describing the
         Dataset.
        :type documentdbcollection_annotations: list[object]
        :param documentdbcollection_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type documentdbcollection_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=documentdbcollection_type, description=documentdbcollection_description, structure=documentdbcollection_structure, schema=documentdbcollection_schema, linked_service_name=documentdbcollection_linked_service_name, parameters=documentdbcollection_parameters, annotations=documentdbcollection_annotations, folder=documentdbcollection_folder, collection_name=documentdbcollection_collection_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_documentdbcollection.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_documentdbcollection.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_drilltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        drilltable_type: str,
        drilltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        drilltable_description: Optional[str] = None,
        drilltable_structure: Optional[object] = None,
        drilltable_schema: Optional[object] = None,
        drilltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        drilltable_annotations: Optional[List[object]] = None,
        drilltable_folder: Optional["models.DatasetFolder"] = None,
        drilltable_table_name: Optional[object] = None,
        drilltable_table: Optional[object] = None,
        drilltable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param drilltable_type: Type of dataset.
        :type drilltable_type: str
        :param drilltable_linked_service_name: Linked service reference.
        :type drilltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param drilltable_description: Dataset description.
        :type drilltable_description: str
        :param drilltable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type drilltable_structure: object
        :param drilltable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type drilltable_schema: object
        :param drilltable_parameters: Parameters for dataset.
        :type drilltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param drilltable_annotations: List of tags that can be used for describing the Dataset.
        :type drilltable_annotations: list[object]
        :param drilltable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type drilltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param drilltable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type drilltable_table_name: object
        :param drilltable_table: The table name of the Drill. Type: string (or Expression with
         resultType string).
        :type drilltable_table: object
        :param drilltable_schema_type_properties_schema: The schema name of the Drill. Type: string (or
         Expression with resultType string).
        :type drilltable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=drilltable_type, description=drilltable_description, structure=drilltable_structure, schema=drilltable_schema, linked_service_name=drilltable_linked_service_name, parameters=drilltable_parameters, annotations=drilltable_annotations, folder=drilltable_folder, table_name=drilltable_table_name, table=drilltable_table, schema_type_properties_schema=drilltable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_drilltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_drilltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_dynamicsaxresource(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        dynamicsaxresource_type: str,
        dynamicsaxresource_linked_service_name: "models.LinkedServiceReference",
        dynamicsaxresource_path: object,
        if_match: Optional[str] = None,
        dynamicsaxresource_description: Optional[str] = None,
        dynamicsaxresource_structure: Optional[object] = None,
        dynamicsaxresource_schema: Optional[object] = None,
        dynamicsaxresource_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        dynamicsaxresource_annotations: Optional[List[object]] = None,
        dynamicsaxresource_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param dynamicsaxresource_type: Type of dataset.
        :type dynamicsaxresource_type: str
        :param dynamicsaxresource_linked_service_name: Linked service reference.
        :type dynamicsaxresource_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param dynamicsaxresource_path: The path of the Dynamics AX OData entity. Type: string (or
         Expression with resultType string).
        :type dynamicsaxresource_path: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param dynamicsaxresource_description: Dataset description.
        :type dynamicsaxresource_description: str
        :param dynamicsaxresource_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type dynamicsaxresource_structure: object
        :param dynamicsaxresource_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type dynamicsaxresource_schema: object
        :param dynamicsaxresource_parameters: Parameters for dataset.
        :type dynamicsaxresource_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param dynamicsaxresource_annotations: List of tags that can be used for describing the
         Dataset.
        :type dynamicsaxresource_annotations: list[object]
        :param dynamicsaxresource_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type dynamicsaxresource_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=dynamicsaxresource_type, description=dynamicsaxresource_description, structure=dynamicsaxresource_structure, schema=dynamicsaxresource_schema, linked_service_name=dynamicsaxresource_linked_service_name, parameters=dynamicsaxresource_parameters, annotations=dynamicsaxresource_annotations, folder=dynamicsaxresource_folder, path=dynamicsaxresource_path)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_dynamicsaxresource.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_dynamicsaxresource.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_dynamicscrmentity(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        dynamicscrmentity_type: str,
        dynamicscrmentity_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        dynamicscrmentity_description: Optional[str] = None,
        dynamicscrmentity_structure: Optional[object] = None,
        dynamicscrmentity_schema: Optional[object] = None,
        dynamicscrmentity_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        dynamicscrmentity_annotations: Optional[List[object]] = None,
        dynamicscrmentity_folder: Optional["models.DatasetFolder"] = None,
        dynamicscrmentity_entity_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param dynamicscrmentity_type: Type of dataset.
        :type dynamicscrmentity_type: str
        :param dynamicscrmentity_linked_service_name: Linked service reference.
        :type dynamicscrmentity_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param dynamicscrmentity_description: Dataset description.
        :type dynamicscrmentity_description: str
        :param dynamicscrmentity_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type dynamicscrmentity_structure: object
        :param dynamicscrmentity_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type dynamicscrmentity_schema: object
        :param dynamicscrmentity_parameters: Parameters for dataset.
        :type dynamicscrmentity_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param dynamicscrmentity_annotations: List of tags that can be used for describing the Dataset.
        :type dynamicscrmentity_annotations: list[object]
        :param dynamicscrmentity_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type dynamicscrmentity_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param dynamicscrmentity_entity_name: The logical name of the entity. Type: string (or
         Expression with resultType string).
        :type dynamicscrmentity_entity_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=dynamicscrmentity_type, description=dynamicscrmentity_description, structure=dynamicscrmentity_structure, schema=dynamicscrmentity_schema, linked_service_name=dynamicscrmentity_linked_service_name, parameters=dynamicscrmentity_parameters, annotations=dynamicscrmentity_annotations, folder=dynamicscrmentity_folder, entity_name=dynamicscrmentity_entity_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_dynamicscrmentity.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_dynamicscrmentity.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_dynamicsentity(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        dynamicsentity_type: str,
        dynamicsentity_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        dynamicsentity_description: Optional[str] = None,
        dynamicsentity_structure: Optional[object] = None,
        dynamicsentity_schema: Optional[object] = None,
        dynamicsentity_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        dynamicsentity_annotations: Optional[List[object]] = None,
        dynamicsentity_folder: Optional["models.DatasetFolder"] = None,
        dynamicsentity_entity_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param dynamicsentity_type: Type of dataset.
        :type dynamicsentity_type: str
        :param dynamicsentity_linked_service_name: Linked service reference.
        :type dynamicsentity_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param dynamicsentity_description: Dataset description.
        :type dynamicsentity_description: str
        :param dynamicsentity_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type dynamicsentity_structure: object
        :param dynamicsentity_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type dynamicsentity_schema: object
        :param dynamicsentity_parameters: Parameters for dataset.
        :type dynamicsentity_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param dynamicsentity_annotations: List of tags that can be used for describing the Dataset.
        :type dynamicsentity_annotations: list[object]
        :param dynamicsentity_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type dynamicsentity_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param dynamicsentity_entity_name: The logical name of the entity. Type: string (or Expression
         with resultType string).
        :type dynamicsentity_entity_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=dynamicsentity_type, description=dynamicsentity_description, structure=dynamicsentity_structure, schema=dynamicsentity_schema, linked_service_name=dynamicsentity_linked_service_name, parameters=dynamicsentity_parameters, annotations=dynamicsentity_annotations, folder=dynamicsentity_folder, entity_name=dynamicsentity_entity_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_dynamicsentity.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_dynamicsentity.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_eloquaobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        eloquaobject_type: str,
        eloquaobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        eloquaobject_description: Optional[str] = None,
        eloquaobject_structure: Optional[object] = None,
        eloquaobject_schema: Optional[object] = None,
        eloquaobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        eloquaobject_annotations: Optional[List[object]] = None,
        eloquaobject_folder: Optional["models.DatasetFolder"] = None,
        eloquaobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param eloquaobject_type: Type of dataset.
        :type eloquaobject_type: str
        :param eloquaobject_linked_service_name: Linked service reference.
        :type eloquaobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param eloquaobject_description: Dataset description.
        :type eloquaobject_description: str
        :param eloquaobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type eloquaobject_structure: object
        :param eloquaobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type eloquaobject_schema: object
        :param eloquaobject_parameters: Parameters for dataset.
        :type eloquaobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param eloquaobject_annotations: List of tags that can be used for describing the Dataset.
        :type eloquaobject_annotations: list[object]
        :param eloquaobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type eloquaobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param eloquaobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type eloquaobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=eloquaobject_type, description=eloquaobject_description, structure=eloquaobject_structure, schema=eloquaobject_schema, linked_service_name=eloquaobject_linked_service_name, parameters=eloquaobject_parameters, annotations=eloquaobject_annotations, folder=eloquaobject_folder, table_name=eloquaobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_eloquaobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_eloquaobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_fileshare(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        fileshare_type: str,
        fileshare_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        fileshare_description: Optional[str] = None,
        fileshare_structure: Optional[object] = None,
        fileshare_schema: Optional[object] = None,
        fileshare_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        fileshare_annotations: Optional[List[object]] = None,
        fileshare_folder: Optional["models.DatasetFolder"] = None,
        fileshare_folder_path: Optional[object] = None,
        fileshare_file_name: Optional[object] = None,
        fileshare_modified_datetime_start: Optional[object] = None,
        fileshare_modified_datetime_end: Optional[object] = None,
        fileshare_format: Optional["models.DatasetStorageFormat"] = None,
        fileshare_file_filter: Optional[object] = None,
        fileshare_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param fileshare_type: Type of dataset.
        :type fileshare_type: str
        :param fileshare_linked_service_name: Linked service reference.
        :type fileshare_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param fileshare_description: Dataset description.
        :type fileshare_description: str
        :param fileshare_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type fileshare_structure: object
        :param fileshare_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type fileshare_schema: object
        :param fileshare_parameters: Parameters for dataset.
        :type fileshare_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param fileshare_annotations: List of tags that can be used for describing the Dataset.
        :type fileshare_annotations: list[object]
        :param fileshare_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type fileshare_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param fileshare_folder_path: The path of the on-premises file system. Type: string (or
         Expression with resultType string).
        :type fileshare_folder_path: object
        :param fileshare_file_name: The name of the on-premises file system. Type: string (or
         Expression with resultType string).
        :type fileshare_file_name: object
        :param fileshare_modified_datetime_start: The start of file's modified datetime. Type: string
         (or Expression with resultType string).
        :type fileshare_modified_datetime_start: object
        :param fileshare_modified_datetime_end: The end of file's modified datetime. Type: string (or
         Expression with resultType string).
        :type fileshare_modified_datetime_end: object
        :param fileshare_format: The format of the files.
        :type fileshare_format: ~azure.mgmt.datafactory.models.DatasetStorageFormat
        :param fileshare_file_filter: Specify a filter to be used to select a subset of files in the
         folderPath rather than all files. Type: string (or Expression with resultType string).
        :type fileshare_file_filter: object
        :param fileshare_compression: The data compression method used for the file system.
        :type fileshare_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=fileshare_type, description=fileshare_description, structure=fileshare_structure, schema=fileshare_schema, linked_service_name=fileshare_linked_service_name, parameters=fileshare_parameters, annotations=fileshare_annotations, folder=fileshare_folder, folder_path=fileshare_folder_path, file_name=fileshare_file_name, modified_datetime_start=fileshare_modified_datetime_start, modified_datetime_end=fileshare_modified_datetime_end, format=fileshare_format, file_filter=fileshare_file_filter, compression=fileshare_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_fileshare.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_fileshare.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_googleadwordsobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        googleadwordsobject_type: str,
        googleadwordsobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        googleadwordsobject_description: Optional[str] = None,
        googleadwordsobject_structure: Optional[object] = None,
        googleadwordsobject_schema: Optional[object] = None,
        googleadwordsobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        googleadwordsobject_annotations: Optional[List[object]] = None,
        googleadwordsobject_folder: Optional["models.DatasetFolder"] = None,
        googleadwordsobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param googleadwordsobject_type: Type of dataset.
        :type googleadwordsobject_type: str
        :param googleadwordsobject_linked_service_name: Linked service reference.
        :type googleadwordsobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param googleadwordsobject_description: Dataset description.
        :type googleadwordsobject_description: str
        :param googleadwordsobject_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type googleadwordsobject_structure: object
        :param googleadwordsobject_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type googleadwordsobject_schema: object
        :param googleadwordsobject_parameters: Parameters for dataset.
        :type googleadwordsobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param googleadwordsobject_annotations: List of tags that can be used for describing the
         Dataset.
        :type googleadwordsobject_annotations: list[object]
        :param googleadwordsobject_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type googleadwordsobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param googleadwordsobject_table_name: The table name. Type: string (or Expression with
         resultType string).
        :type googleadwordsobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=googleadwordsobject_type, description=googleadwordsobject_description, structure=googleadwordsobject_structure, schema=googleadwordsobject_schema, linked_service_name=googleadwordsobject_linked_service_name, parameters=googleadwordsobject_parameters, annotations=googleadwordsobject_annotations, folder=googleadwordsobject_folder, table_name=googleadwordsobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_googleadwordsobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_googleadwordsobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_googlebigqueryobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        googlebigqueryobject_type: str,
        googlebigqueryobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        googlebigqueryobject_description: Optional[str] = None,
        googlebigqueryobject_structure: Optional[object] = None,
        googlebigqueryobject_schema: Optional[object] = None,
        googlebigqueryobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        googlebigqueryobject_annotations: Optional[List[object]] = None,
        googlebigqueryobject_folder: Optional["models.DatasetFolder"] = None,
        googlebigqueryobject_table_name: Optional[object] = None,
        googlebigqueryobject_table: Optional[object] = None,
        googlebigqueryobject_dataset: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param googlebigqueryobject_type: Type of dataset.
        :type googlebigqueryobject_type: str
        :param googlebigqueryobject_linked_service_name: Linked service reference.
        :type googlebigqueryobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param googlebigqueryobject_description: Dataset description.
        :type googlebigqueryobject_description: str
        :param googlebigqueryobject_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type googlebigqueryobject_structure: object
        :param googlebigqueryobject_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type googlebigqueryobject_schema: object
        :param googlebigqueryobject_parameters: Parameters for dataset.
        :type googlebigqueryobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param googlebigqueryobject_annotations: List of tags that can be used for describing the
         Dataset.
        :type googlebigqueryobject_annotations: list[object]
        :param googlebigqueryobject_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type googlebigqueryobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param googlebigqueryobject_table_name: This property will be retired. Please consider using
         database + table properties instead.
        :type googlebigqueryobject_table_name: object
        :param googlebigqueryobject_table: The table name of the Google BigQuery. Type: string (or
         Expression with resultType string).
        :type googlebigqueryobject_table: object
        :param googlebigqueryobject_dataset: The database name of the Google BigQuery. Type: string (or
         Expression with resultType string).
        :type googlebigqueryobject_dataset: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=googlebigqueryobject_type, description=googlebigqueryobject_description, structure=googlebigqueryobject_structure, schema=googlebigqueryobject_schema, linked_service_name=googlebigqueryobject_linked_service_name, parameters=googlebigqueryobject_parameters, annotations=googlebigqueryobject_annotations, folder=googlebigqueryobject_folder, table_name=googlebigqueryobject_table_name, table=googlebigqueryobject_table, dataset=googlebigqueryobject_dataset)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_googlebigqueryobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_googlebigqueryobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_greenplumtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        greenplumtable_type: str,
        greenplumtable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        greenplumtable_description: Optional[str] = None,
        greenplumtable_structure: Optional[object] = None,
        greenplumtable_schema: Optional[object] = None,
        greenplumtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        greenplumtable_annotations: Optional[List[object]] = None,
        greenplumtable_folder: Optional["models.DatasetFolder"] = None,
        greenplumtable_table_name: Optional[object] = None,
        greenplumtable_table: Optional[object] = None,
        greenplumtable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param greenplumtable_type: Type of dataset.
        :type greenplumtable_type: str
        :param greenplumtable_linked_service_name: Linked service reference.
        :type greenplumtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param greenplumtable_description: Dataset description.
        :type greenplumtable_description: str
        :param greenplumtable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type greenplumtable_structure: object
        :param greenplumtable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type greenplumtable_schema: object
        :param greenplumtable_parameters: Parameters for dataset.
        :type greenplumtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param greenplumtable_annotations: List of tags that can be used for describing the Dataset.
        :type greenplumtable_annotations: list[object]
        :param greenplumtable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type greenplumtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param greenplumtable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type greenplumtable_table_name: object
        :param greenplumtable_table: The table name of Greenplum. Type: string (or Expression with
         resultType string).
        :type greenplumtable_table: object
        :param greenplumtable_schema_type_properties_schema: The schema name of Greenplum. Type: string
         (or Expression with resultType string).
        :type greenplumtable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=greenplumtable_type, description=greenplumtable_description, structure=greenplumtable_structure, schema=greenplumtable_schema, linked_service_name=greenplumtable_linked_service_name, parameters=greenplumtable_parameters, annotations=greenplumtable_annotations, folder=greenplumtable_folder, table_name=greenplumtable_table_name, table=greenplumtable_table, schema_type_properties_schema=greenplumtable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_greenplumtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_greenplumtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_hbaseobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        hbaseobject_type: str,
        hbaseobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        hbaseobject_description: Optional[str] = None,
        hbaseobject_structure: Optional[object] = None,
        hbaseobject_schema: Optional[object] = None,
        hbaseobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        hbaseobject_annotations: Optional[List[object]] = None,
        hbaseobject_folder: Optional["models.DatasetFolder"] = None,
        hbaseobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param hbaseobject_type: Type of dataset.
        :type hbaseobject_type: str
        :param hbaseobject_linked_service_name: Linked service reference.
        :type hbaseobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param hbaseobject_description: Dataset description.
        :type hbaseobject_description: str
        :param hbaseobject_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type hbaseobject_structure: object
        :param hbaseobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type hbaseobject_schema: object
        :param hbaseobject_parameters: Parameters for dataset.
        :type hbaseobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param hbaseobject_annotations: List of tags that can be used for describing the Dataset.
        :type hbaseobject_annotations: list[object]
        :param hbaseobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type hbaseobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param hbaseobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type hbaseobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=hbaseobject_type, description=hbaseobject_description, structure=hbaseobject_structure, schema=hbaseobject_schema, linked_service_name=hbaseobject_linked_service_name, parameters=hbaseobject_parameters, annotations=hbaseobject_annotations, folder=hbaseobject_folder, table_name=hbaseobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_hbaseobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_hbaseobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_hiveobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        hiveobject_type: str,
        hiveobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        hiveobject_description: Optional[str] = None,
        hiveobject_structure: Optional[object] = None,
        hiveobject_schema: Optional[object] = None,
        hiveobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        hiveobject_annotations: Optional[List[object]] = None,
        hiveobject_folder: Optional["models.DatasetFolder"] = None,
        hiveobject_table_name: Optional[object] = None,
        hiveobject_table: Optional[object] = None,
        hiveobject_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param hiveobject_type: Type of dataset.
        :type hiveobject_type: str
        :param hiveobject_linked_service_name: Linked service reference.
        :type hiveobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param hiveobject_description: Dataset description.
        :type hiveobject_description: str
        :param hiveobject_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type hiveobject_structure: object
        :param hiveobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type hiveobject_schema: object
        :param hiveobject_parameters: Parameters for dataset.
        :type hiveobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param hiveobject_annotations: List of tags that can be used for describing the Dataset.
        :type hiveobject_annotations: list[object]
        :param hiveobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type hiveobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param hiveobject_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type hiveobject_table_name: object
        :param hiveobject_table: The table name of the Hive. Type: string (or Expression with
         resultType string).
        :type hiveobject_table: object
        :param hiveobject_schema_type_properties_schema: The schema name of the Hive. Type: string (or
         Expression with resultType string).
        :type hiveobject_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=hiveobject_type, description=hiveobject_description, structure=hiveobject_structure, schema=hiveobject_schema, linked_service_name=hiveobject_linked_service_name, parameters=hiveobject_parameters, annotations=hiveobject_annotations, folder=hiveobject_folder, table_name=hiveobject_table_name, table=hiveobject_table, schema_type_properties_schema=hiveobject_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_hiveobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_hiveobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_httpfile(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        httpfile_type: str,
        httpfile_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        httpfile_description: Optional[str] = None,
        httpfile_structure: Optional[object] = None,
        httpfile_schema: Optional[object] = None,
        httpfile_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        httpfile_annotations: Optional[List[object]] = None,
        httpfile_folder: Optional["models.DatasetFolder"] = None,
        httpfile_relative_url: Optional[object] = None,
        httpfile_request_method: Optional[object] = None,
        httpfile_request_body: Optional[object] = None,
        httpfile_additional_headers: Optional[object] = None,
        httpfile_format: Optional["models.DatasetStorageFormat"] = None,
        httpfile_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param httpfile_type: Type of dataset.
        :type httpfile_type: str
        :param httpfile_linked_service_name: Linked service reference.
        :type httpfile_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param httpfile_description: Dataset description.
        :type httpfile_description: str
        :param httpfile_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type httpfile_structure: object
        :param httpfile_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type httpfile_schema: object
        :param httpfile_parameters: Parameters for dataset.
        :type httpfile_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param httpfile_annotations: List of tags that can be used for describing the Dataset.
        :type httpfile_annotations: list[object]
        :param httpfile_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type httpfile_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param httpfile_relative_url: The relative URL based on the URL in the HttpLinkedService refers
         to an HTTP file Type: string (or Expression with resultType string).
        :type httpfile_relative_url: object
        :param httpfile_request_method: The HTTP method for the HTTP request. Type: string (or
         Expression with resultType string).
        :type httpfile_request_method: object
        :param httpfile_request_body: The body for the HTTP request. Type: string (or Expression with
         resultType string).
        :type httpfile_request_body: object
        :param httpfile_additional_headers: The headers for the HTTP Request. e.g. request-header-
         name-1:request-header-value-1
         ...
         request-header-name-n:request-header-value-n Type: string (or Expression with resultType
         string).
        :type httpfile_additional_headers: object
        :param httpfile_format: The format of files.
        :type httpfile_format: ~azure.mgmt.datafactory.models.DatasetStorageFormat
        :param httpfile_compression: The data compression method used on files.
        :type httpfile_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=httpfile_type, description=httpfile_description, structure=httpfile_structure, schema=httpfile_schema, linked_service_name=httpfile_linked_service_name, parameters=httpfile_parameters, annotations=httpfile_annotations, folder=httpfile_folder, relative_url=httpfile_relative_url, request_method=httpfile_request_method, request_body=httpfile_request_body, additional_headers=httpfile_additional_headers, format=httpfile_format, compression=httpfile_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_httpfile.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_httpfile.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_hubspotobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        hubspotobject_type: str,
        hubspotobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        hubspotobject_description: Optional[str] = None,
        hubspotobject_structure: Optional[object] = None,
        hubspotobject_schema: Optional[object] = None,
        hubspotobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        hubspotobject_annotations: Optional[List[object]] = None,
        hubspotobject_folder: Optional["models.DatasetFolder"] = None,
        hubspotobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param hubspotobject_type: Type of dataset.
        :type hubspotobject_type: str
        :param hubspotobject_linked_service_name: Linked service reference.
        :type hubspotobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param hubspotobject_description: Dataset description.
        :type hubspotobject_description: str
        :param hubspotobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type hubspotobject_structure: object
        :param hubspotobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type hubspotobject_schema: object
        :param hubspotobject_parameters: Parameters for dataset.
        :type hubspotobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param hubspotobject_annotations: List of tags that can be used for describing the Dataset.
        :type hubspotobject_annotations: list[object]
        :param hubspotobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type hubspotobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param hubspotobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type hubspotobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=hubspotobject_type, description=hubspotobject_description, structure=hubspotobject_structure, schema=hubspotobject_schema, linked_service_name=hubspotobject_linked_service_name, parameters=hubspotobject_parameters, annotations=hubspotobject_annotations, folder=hubspotobject_folder, table_name=hubspotobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_hubspotobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_hubspotobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_impalaobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        impalaobject_type: str,
        impalaobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        impalaobject_description: Optional[str] = None,
        impalaobject_structure: Optional[object] = None,
        impalaobject_schema: Optional[object] = None,
        impalaobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        impalaobject_annotations: Optional[List[object]] = None,
        impalaobject_folder: Optional["models.DatasetFolder"] = None,
        impalaobject_table_name: Optional[object] = None,
        impalaobject_table: Optional[object] = None,
        impalaobject_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param impalaobject_type: Type of dataset.
        :type impalaobject_type: str
        :param impalaobject_linked_service_name: Linked service reference.
        :type impalaobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param impalaobject_description: Dataset description.
        :type impalaobject_description: str
        :param impalaobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type impalaobject_structure: object
        :param impalaobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type impalaobject_schema: object
        :param impalaobject_parameters: Parameters for dataset.
        :type impalaobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param impalaobject_annotations: List of tags that can be used for describing the Dataset.
        :type impalaobject_annotations: list[object]
        :param impalaobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type impalaobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param impalaobject_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type impalaobject_table_name: object
        :param impalaobject_table: The table name of the Impala. Type: string (or Expression with
         resultType string).
        :type impalaobject_table: object
        :param impalaobject_schema_type_properties_schema: The schema name of the Impala. Type: string
         (or Expression with resultType string).
        :type impalaobject_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=impalaobject_type, description=impalaobject_description, structure=impalaobject_structure, schema=impalaobject_schema, linked_service_name=impalaobject_linked_service_name, parameters=impalaobject_parameters, annotations=impalaobject_annotations, folder=impalaobject_folder, table_name=impalaobject_table_name, table=impalaobject_table, schema_type_properties_schema=impalaobject_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_impalaobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_impalaobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_informixtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        informixtable_type: str,
        informixtable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        informixtable_description: Optional[str] = None,
        informixtable_structure: Optional[object] = None,
        informixtable_schema: Optional[object] = None,
        informixtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        informixtable_annotations: Optional[List[object]] = None,
        informixtable_folder: Optional["models.DatasetFolder"] = None,
        informixtable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param informixtable_type: Type of dataset.
        :type informixtable_type: str
        :param informixtable_linked_service_name: Linked service reference.
        :type informixtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param informixtable_description: Dataset description.
        :type informixtable_description: str
        :param informixtable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type informixtable_structure: object
        :param informixtable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type informixtable_schema: object
        :param informixtable_parameters: Parameters for dataset.
        :type informixtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param informixtable_annotations: List of tags that can be used for describing the Dataset.
        :type informixtable_annotations: list[object]
        :param informixtable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type informixtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param informixtable_table_name: The Informix table name. Type: string (or Expression with
         resultType string).
        :type informixtable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=informixtable_type, description=informixtable_description, structure=informixtable_structure, schema=informixtable_schema, linked_service_name=informixtable_linked_service_name, parameters=informixtable_parameters, annotations=informixtable_annotations, folder=informixtable_folder, table_name=informixtable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_informixtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_informixtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_jiraobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        jiraobject_type: str,
        jiraobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        jiraobject_description: Optional[str] = None,
        jiraobject_structure: Optional[object] = None,
        jiraobject_schema: Optional[object] = None,
        jiraobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        jiraobject_annotations: Optional[List[object]] = None,
        jiraobject_folder: Optional["models.DatasetFolder"] = None,
        jiraobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param jiraobject_type: Type of dataset.
        :type jiraobject_type: str
        :param jiraobject_linked_service_name: Linked service reference.
        :type jiraobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param jiraobject_description: Dataset description.
        :type jiraobject_description: str
        :param jiraobject_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type jiraobject_structure: object
        :param jiraobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type jiraobject_schema: object
        :param jiraobject_parameters: Parameters for dataset.
        :type jiraobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param jiraobject_annotations: List of tags that can be used for describing the Dataset.
        :type jiraobject_annotations: list[object]
        :param jiraobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type jiraobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param jiraobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type jiraobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=jiraobject_type, description=jiraobject_description, structure=jiraobject_structure, schema=jiraobject_schema, linked_service_name=jiraobject_linked_service_name, parameters=jiraobject_parameters, annotations=jiraobject_annotations, folder=jiraobject_folder, table_name=jiraobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_jiraobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_jiraobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_json(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        json_type: str,
        json_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        json_description: Optional[str] = None,
        json_structure: Optional[object] = None,
        json_schema: Optional[object] = None,
        json_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        json_annotations: Optional[List[object]] = None,
        json_folder: Optional["models.DatasetFolder"] = None,
        json_location: Optional["models.DatasetLocation"] = None,
        json_encoding_name: Optional[object] = None,
        json_compression: Optional["models.DatasetCompression"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param json_type: Type of dataset.
        :type json_type: str
        :param json_linked_service_name: Linked service reference.
        :type json_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param json_description: Dataset description.
        :type json_description: str
        :param json_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type json_structure: object
        :param json_schema: Columns that define the physical type schema of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type json_schema: object
        :param json_parameters: Parameters for dataset.
        :type json_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param json_annotations: List of tags that can be used for describing the Dataset.
        :type json_annotations: list[object]
        :param json_folder: The folder that this Dataset is in. If not specified, Dataset will appear
         at the root level.
        :type json_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param json_location: The location of the json data storage.
        :type json_location: ~azure.mgmt.datafactory.models.DatasetLocation
        :param json_encoding_name: The code page name of the preferred encoding. If not specified, the
         default value is UTF-8, unless BOM denotes another Unicode encoding. Refer to the name column
         of the table in the following link to set supported values:
         https://msdn.microsoft.com/library/system.text.encoding.aspx. Type: string (or Expression with
         resultType string).
        :type json_encoding_name: object
        :param json_compression: The data compression method used for the json dataset.
        :type json_compression: ~azure.mgmt.datafactory.models.DatasetCompression
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=json_type, description=json_description, structure=json_structure, schema=json_schema, linked_service_name=json_linked_service_name, parameters=json_parameters, annotations=json_annotations, folder=json_folder, location=json_location, encoding_name=json_encoding_name, compression=json_compression)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_json.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_json.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_magentoobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        magentoobject_type: str,
        magentoobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        magentoobject_description: Optional[str] = None,
        magentoobject_structure: Optional[object] = None,
        magentoobject_schema: Optional[object] = None,
        magentoobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        magentoobject_annotations: Optional[List[object]] = None,
        magentoobject_folder: Optional["models.DatasetFolder"] = None,
        magentoobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param magentoobject_type: Type of dataset.
        :type magentoobject_type: str
        :param magentoobject_linked_service_name: Linked service reference.
        :type magentoobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param magentoobject_description: Dataset description.
        :type magentoobject_description: str
        :param magentoobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type magentoobject_structure: object
        :param magentoobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type magentoobject_schema: object
        :param magentoobject_parameters: Parameters for dataset.
        :type magentoobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param magentoobject_annotations: List of tags that can be used for describing the Dataset.
        :type magentoobject_annotations: list[object]
        :param magentoobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type magentoobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param magentoobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type magentoobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=magentoobject_type, description=magentoobject_description, structure=magentoobject_structure, schema=magentoobject_schema, linked_service_name=magentoobject_linked_service_name, parameters=magentoobject_parameters, annotations=magentoobject_annotations, folder=magentoobject_folder, table_name=magentoobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_magentoobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_magentoobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_mariadbtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        mariadbtable_type: str,
        mariadbtable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        mariadbtable_description: Optional[str] = None,
        mariadbtable_structure: Optional[object] = None,
        mariadbtable_schema: Optional[object] = None,
        mariadbtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        mariadbtable_annotations: Optional[List[object]] = None,
        mariadbtable_folder: Optional["models.DatasetFolder"] = None,
        mariadbtable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param mariadbtable_type: Type of dataset.
        :type mariadbtable_type: str
        :param mariadbtable_linked_service_name: Linked service reference.
        :type mariadbtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param mariadbtable_description: Dataset description.
        :type mariadbtable_description: str
        :param mariadbtable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type mariadbtable_structure: object
        :param mariadbtable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type mariadbtable_schema: object
        :param mariadbtable_parameters: Parameters for dataset.
        :type mariadbtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param mariadbtable_annotations: List of tags that can be used for describing the Dataset.
        :type mariadbtable_annotations: list[object]
        :param mariadbtable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type mariadbtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param mariadbtable_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type mariadbtable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=mariadbtable_type, description=mariadbtable_description, structure=mariadbtable_structure, schema=mariadbtable_schema, linked_service_name=mariadbtable_linked_service_name, parameters=mariadbtable_parameters, annotations=mariadbtable_annotations, folder=mariadbtable_folder, table_name=mariadbtable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_mariadbtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_mariadbtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_marketoobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        marketoobject_type: str,
        marketoobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        marketoobject_description: Optional[str] = None,
        marketoobject_structure: Optional[object] = None,
        marketoobject_schema: Optional[object] = None,
        marketoobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        marketoobject_annotations: Optional[List[object]] = None,
        marketoobject_folder: Optional["models.DatasetFolder"] = None,
        marketoobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param marketoobject_type: Type of dataset.
        :type marketoobject_type: str
        :param marketoobject_linked_service_name: Linked service reference.
        :type marketoobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param marketoobject_description: Dataset description.
        :type marketoobject_description: str
        :param marketoobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type marketoobject_structure: object
        :param marketoobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type marketoobject_schema: object
        :param marketoobject_parameters: Parameters for dataset.
        :type marketoobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param marketoobject_annotations: List of tags that can be used for describing the Dataset.
        :type marketoobject_annotations: list[object]
        :param marketoobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type marketoobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param marketoobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type marketoobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=marketoobject_type, description=marketoobject_description, structure=marketoobject_structure, schema=marketoobject_schema, linked_service_name=marketoobject_linked_service_name, parameters=marketoobject_parameters, annotations=marketoobject_annotations, folder=marketoobject_folder, table_name=marketoobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_marketoobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_marketoobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_microsoftaccesstable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        microsoftaccesstable_type: str,
        microsoftaccesstable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        microsoftaccesstable_description: Optional[str] = None,
        microsoftaccesstable_structure: Optional[object] = None,
        microsoftaccesstable_schema: Optional[object] = None,
        microsoftaccesstable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        microsoftaccesstable_annotations: Optional[List[object]] = None,
        microsoftaccesstable_folder: Optional["models.DatasetFolder"] = None,
        microsoftaccesstable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param microsoftaccesstable_type: Type of dataset.
        :type microsoftaccesstable_type: str
        :param microsoftaccesstable_linked_service_name: Linked service reference.
        :type microsoftaccesstable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param microsoftaccesstable_description: Dataset description.
        :type microsoftaccesstable_description: str
        :param microsoftaccesstable_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type microsoftaccesstable_structure: object
        :param microsoftaccesstable_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type microsoftaccesstable_schema: object
        :param microsoftaccesstable_parameters: Parameters for dataset.
        :type microsoftaccesstable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param microsoftaccesstable_annotations: List of tags that can be used for describing the
         Dataset.
        :type microsoftaccesstable_annotations: list[object]
        :param microsoftaccesstable_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type microsoftaccesstable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param microsoftaccesstable_table_name: The Microsoft Access table name. Type: string (or
         Expression with resultType string).
        :type microsoftaccesstable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=microsoftaccesstable_type, description=microsoftaccesstable_description, structure=microsoftaccesstable_structure, schema=microsoftaccesstable_schema, linked_service_name=microsoftaccesstable_linked_service_name, parameters=microsoftaccesstable_parameters, annotations=microsoftaccesstable_annotations, folder=microsoftaccesstable_folder, table_name=microsoftaccesstable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_microsoftaccesstable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_microsoftaccesstable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_mongodbcollection(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        mongodbcollection_type: str,
        mongodbcollection_linked_service_name: "models.LinkedServiceReference",
        mongodbcollection_collection_name: object,
        if_match: Optional[str] = None,
        mongodbcollection_description: Optional[str] = None,
        mongodbcollection_structure: Optional[object] = None,
        mongodbcollection_schema: Optional[object] = None,
        mongodbcollection_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        mongodbcollection_annotations: Optional[List[object]] = None,
        mongodbcollection_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param mongodbcollection_type: Type of dataset.
        :type mongodbcollection_type: str
        :param mongodbcollection_linked_service_name: Linked service reference.
        :type mongodbcollection_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param mongodbcollection_collection_name: The table name of the MongoDB database. Type: string
         (or Expression with resultType string).
        :type mongodbcollection_collection_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param mongodbcollection_description: Dataset description.
        :type mongodbcollection_description: str
        :param mongodbcollection_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type mongodbcollection_structure: object
        :param mongodbcollection_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type mongodbcollection_schema: object
        :param mongodbcollection_parameters: Parameters for dataset.
        :type mongodbcollection_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param mongodbcollection_annotations: List of tags that can be used for describing the Dataset.
        :type mongodbcollection_annotations: list[object]
        :param mongodbcollection_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type mongodbcollection_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=mongodbcollection_type, description=mongodbcollection_description, structure=mongodbcollection_structure, schema=mongodbcollection_schema, linked_service_name=mongodbcollection_linked_service_name, parameters=mongodbcollection_parameters, annotations=mongodbcollection_annotations, folder=mongodbcollection_folder, collection_name=mongodbcollection_collection_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_mongodbcollection.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_mongodbcollection.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_mongodbv2collection(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        mongodbv2collection_type: str,
        mongodbv2collection_linked_service_name: "models.LinkedServiceReference",
        mongodbv2collection_collection: object,
        if_match: Optional[str] = None,
        mongodbv2collection_description: Optional[str] = None,
        mongodbv2collection_structure: Optional[object] = None,
        mongodbv2collection_schema: Optional[object] = None,
        mongodbv2collection_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        mongodbv2collection_annotations: Optional[List[object]] = None,
        mongodbv2collection_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param mongodbv2collection_type: Type of dataset.
        :type mongodbv2collection_type: str
        :param mongodbv2collection_linked_service_name: Linked service reference.
        :type mongodbv2collection_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param mongodbv2collection_collection: The collection name of the MongoDB database. Type:
         string (or Expression with resultType string).
        :type mongodbv2collection_collection: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param mongodbv2collection_description: Dataset description.
        :type mongodbv2collection_description: str
        :param mongodbv2collection_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type mongodbv2collection_structure: object
        :param mongodbv2collection_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type mongodbv2collection_schema: object
        :param mongodbv2collection_parameters: Parameters for dataset.
        :type mongodbv2collection_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param mongodbv2collection_annotations: List of tags that can be used for describing the
         Dataset.
        :type mongodbv2collection_annotations: list[object]
        :param mongodbv2collection_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type mongodbv2collection_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=mongodbv2collection_type, description=mongodbv2collection_description, structure=mongodbv2collection_structure, schema=mongodbv2collection_schema, linked_service_name=mongodbv2collection_linked_service_name, parameters=mongodbv2collection_parameters, annotations=mongodbv2collection_annotations, folder=mongodbv2collection_folder, collection=mongodbv2collection_collection)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_mongodbv2collection.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_mongodbv2collection.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_mysqltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        mysqltable_type: str,
        mysqltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        mysqltable_description: Optional[str] = None,
        mysqltable_structure: Optional[object] = None,
        mysqltable_schema: Optional[object] = None,
        mysqltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        mysqltable_annotations: Optional[List[object]] = None,
        mysqltable_folder: Optional["models.DatasetFolder"] = None,
        mysqltable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param mysqltable_type: Type of dataset.
        :type mysqltable_type: str
        :param mysqltable_linked_service_name: Linked service reference.
        :type mysqltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param mysqltable_description: Dataset description.
        :type mysqltable_description: str
        :param mysqltable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type mysqltable_structure: object
        :param mysqltable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type mysqltable_schema: object
        :param mysqltable_parameters: Parameters for dataset.
        :type mysqltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param mysqltable_annotations: List of tags that can be used for describing the Dataset.
        :type mysqltable_annotations: list[object]
        :param mysqltable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type mysqltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param mysqltable_table_name: The MySQL table name. Type: string (or Expression with resultType
         string).
        :type mysqltable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=mysqltable_type, description=mysqltable_description, structure=mysqltable_structure, schema=mysqltable_schema, linked_service_name=mysqltable_linked_service_name, parameters=mysqltable_parameters, annotations=mysqltable_annotations, folder=mysqltable_folder, table_name=mysqltable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_mysqltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_mysqltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_netezzatable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        netezzatable_type: str,
        netezzatable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        netezzatable_description: Optional[str] = None,
        netezzatable_structure: Optional[object] = None,
        netezzatable_schema: Optional[object] = None,
        netezzatable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        netezzatable_annotations: Optional[List[object]] = None,
        netezzatable_folder: Optional["models.DatasetFolder"] = None,
        netezzatable_table_name: Optional[object] = None,
        netezzatable_table: Optional[object] = None,
        netezzatable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param netezzatable_type: Type of dataset.
        :type netezzatable_type: str
        :param netezzatable_linked_service_name: Linked service reference.
        :type netezzatable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param netezzatable_description: Dataset description.
        :type netezzatable_description: str
        :param netezzatable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type netezzatable_structure: object
        :param netezzatable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type netezzatable_schema: object
        :param netezzatable_parameters: Parameters for dataset.
        :type netezzatable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param netezzatable_annotations: List of tags that can be used for describing the Dataset.
        :type netezzatable_annotations: list[object]
        :param netezzatable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type netezzatable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param netezzatable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type netezzatable_table_name: object
        :param netezzatable_table: The table name of the Netezza. Type: string (or Expression with
         resultType string).
        :type netezzatable_table: object
        :param netezzatable_schema_type_properties_schema: The schema name of the Netezza. Type: string
         (or Expression with resultType string).
        :type netezzatable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=netezzatable_type, description=netezzatable_description, structure=netezzatable_structure, schema=netezzatable_schema, linked_service_name=netezzatable_linked_service_name, parameters=netezzatable_parameters, annotations=netezzatable_annotations, folder=netezzatable_folder, table_name=netezzatable_table_name, table=netezzatable_table, schema_type_properties_schema=netezzatable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_netezzatable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_netezzatable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_odataresource(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        odataresource_type: str,
        odataresource_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        odataresource_description: Optional[str] = None,
        odataresource_structure: Optional[object] = None,
        odataresource_schema: Optional[object] = None,
        odataresource_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        odataresource_annotations: Optional[List[object]] = None,
        odataresource_folder: Optional["models.DatasetFolder"] = None,
        odataresource_path: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param odataresource_type: Type of dataset.
        :type odataresource_type: str
        :param odataresource_linked_service_name: Linked service reference.
        :type odataresource_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param odataresource_description: Dataset description.
        :type odataresource_description: str
        :param odataresource_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type odataresource_structure: object
        :param odataresource_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type odataresource_schema: object
        :param odataresource_parameters: Parameters for dataset.
        :type odataresource_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param odataresource_annotations: List of tags that can be used for describing the Dataset.
        :type odataresource_annotations: list[object]
        :param odataresource_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type odataresource_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param odataresource_path: The OData resource path. Type: string (or Expression with resultType
         string).
        :type odataresource_path: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=odataresource_type, description=odataresource_description, structure=odataresource_structure, schema=odataresource_schema, linked_service_name=odataresource_linked_service_name, parameters=odataresource_parameters, annotations=odataresource_annotations, folder=odataresource_folder, path=odataresource_path)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_odataresource.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_odataresource.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_odbctable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        odbctable_type: str,
        odbctable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        odbctable_description: Optional[str] = None,
        odbctable_structure: Optional[object] = None,
        odbctable_schema: Optional[object] = None,
        odbctable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        odbctable_annotations: Optional[List[object]] = None,
        odbctable_folder: Optional["models.DatasetFolder"] = None,
        odbctable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param odbctable_type: Type of dataset.
        :type odbctable_type: str
        :param odbctable_linked_service_name: Linked service reference.
        :type odbctable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param odbctable_description: Dataset description.
        :type odbctable_description: str
        :param odbctable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type odbctable_structure: object
        :param odbctable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type odbctable_schema: object
        :param odbctable_parameters: Parameters for dataset.
        :type odbctable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param odbctable_annotations: List of tags that can be used for describing the Dataset.
        :type odbctable_annotations: list[object]
        :param odbctable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type odbctable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param odbctable_table_name: The ODBC table name. Type: string (or Expression with resultType
         string).
        :type odbctable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=odbctable_type, description=odbctable_description, structure=odbctable_structure, schema=odbctable_schema, linked_service_name=odbctable_linked_service_name, parameters=odbctable_parameters, annotations=odbctable_annotations, folder=odbctable_folder, table_name=odbctable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_odbctable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_odbctable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_office365table(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        office365table_type: str,
        office365table_linked_service_name: "models.LinkedServiceReference",
        office365table_table_name: object,
        if_match: Optional[str] = None,
        office365table_description: Optional[str] = None,
        office365table_structure: Optional[object] = None,
        office365table_schema: Optional[object] = None,
        office365table_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        office365table_annotations: Optional[List[object]] = None,
        office365table_folder: Optional["models.DatasetFolder"] = None,
        office365table_predicate: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param office365table_type: Type of dataset.
        :type office365table_type: str
        :param office365table_linked_service_name: Linked service reference.
        :type office365table_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param office365table_table_name: Name of the dataset to extract from Office 365. Type: string
         (or Expression with resultType string).
        :type office365table_table_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param office365table_description: Dataset description.
        :type office365table_description: str
        :param office365table_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type office365table_structure: object
        :param office365table_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type office365table_schema: object
        :param office365table_parameters: Parameters for dataset.
        :type office365table_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param office365table_annotations: List of tags that can be used for describing the Dataset.
        :type office365table_annotations: list[object]
        :param office365table_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type office365table_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param office365table_predicate: A predicate expression that can be used to filter the specific
         rows to extract from Office 365. Type: string (or Expression with resultType string).
        :type office365table_predicate: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=office365table_type, description=office365table_description, structure=office365table_structure, schema=office365table_schema, linked_service_name=office365table_linked_service_name, parameters=office365table_parameters, annotations=office365table_annotations, folder=office365table_folder, table_name=office365table_table_name, predicate=office365table_predicate)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_office365table.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_office365table.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_oracleservicecloudobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        oracleservicecloudobject_type: str,
        oracleservicecloudobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        oracleservicecloudobject_description: Optional[str] = None,
        oracleservicecloudobject_structure: Optional[object] = None,
        oracleservicecloudobject_schema: Optional[object] = None,
        oracleservicecloudobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        oracleservicecloudobject_annotations: Optional[List[object]] = None,
        oracleservicecloudobject_folder: Optional["models.DatasetFolder"] = None,
        oracleservicecloudobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param oracleservicecloudobject_type: Type of dataset.
        :type oracleservicecloudobject_type: str
        :param oracleservicecloudobject_linked_service_name: Linked service reference.
        :type oracleservicecloudobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param oracleservicecloudobject_description: Dataset description.
        :type oracleservicecloudobject_description: str
        :param oracleservicecloudobject_structure: Columns that define the structure of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type oracleservicecloudobject_structure: object
        :param oracleservicecloudobject_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type oracleservicecloudobject_schema: object
        :param oracleservicecloudobject_parameters: Parameters for dataset.
        :type oracleservicecloudobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param oracleservicecloudobject_annotations: List of tags that can be used for describing the
         Dataset.
        :type oracleservicecloudobject_annotations: list[object]
        :param oracleservicecloudobject_folder: The folder that this Dataset is in. If not specified,
         Dataset will appear at the root level.
        :type oracleservicecloudobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param oracleservicecloudobject_table_name: The table name. Type: string (or Expression with
         resultType string).
        :type oracleservicecloudobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=oracleservicecloudobject_type, description=oracleservicecloudobject_description, structure=oracleservicecloudobject_structure, schema=oracleservicecloudobject_schema, linked_service_name=oracleservicecloudobject_linked_service_name, parameters=oracleservicecloudobject_parameters, annotations=oracleservicecloudobject_annotations, folder=oracleservicecloudobject_folder, table_name=oracleservicecloudobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_oracleservicecloudobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_oracleservicecloudobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_oracletable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        oracletable_type: str,
        oracletable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        oracletable_description: Optional[str] = None,
        oracletable_structure: Optional[object] = None,
        oracletable_schema: Optional[object] = None,
        oracletable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        oracletable_annotations: Optional[List[object]] = None,
        oracletable_folder: Optional["models.DatasetFolder"] = None,
        oracletable_table_name: Optional[object] = None,
        oracletable_schema_type_properties_schema: Optional[object] = None,
        oracletable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param oracletable_type: Type of dataset.
        :type oracletable_type: str
        :param oracletable_linked_service_name: Linked service reference.
        :type oracletable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param oracletable_description: Dataset description.
        :type oracletable_description: str
        :param oracletable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type oracletable_structure: object
        :param oracletable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type oracletable_schema: object
        :param oracletable_parameters: Parameters for dataset.
        :type oracletable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param oracletable_annotations: List of tags that can be used for describing the Dataset.
        :type oracletable_annotations: list[object]
        :param oracletable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type oracletable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param oracletable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type oracletable_table_name: object
        :param oracletable_schema_type_properties_schema: The schema name of the on-premises Oracle
         database. Type: string (or Expression with resultType string).
        :type oracletable_schema_type_properties_schema: object
        :param oracletable_table: The table name of the on-premises Oracle database. Type: string (or
         Expression with resultType string).
        :type oracletable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=oracletable_type, description=oracletable_description, structure=oracletable_structure, schema=oracletable_schema, linked_service_name=oracletable_linked_service_name, parameters=oracletable_parameters, annotations=oracletable_annotations, folder=oracletable_folder, table_name=oracletable_table_name, schema_type_properties_schema=oracletable_schema_type_properties_schema, table=oracletable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_oracletable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_oracletable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_orc(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        orc_type: str,
        orc_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        orc_description: Optional[str] = None,
        orc_structure: Optional[object] = None,
        orc_schema: Optional[object] = None,
        orc_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        orc_annotations: Optional[List[object]] = None,
        orc_folder: Optional["models.DatasetFolder"] = None,
        orc_location: Optional["models.DatasetLocation"] = None,
        orc_orc_compression_codec: Optional[Union[str, "models.OrcCompressionCodec"]] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param orc_type: Type of dataset.
        :type orc_type: str
        :param orc_linked_service_name: Linked service reference.
        :type orc_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param orc_description: Dataset description.
        :type orc_description: str
        :param orc_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type orc_structure: object
        :param orc_schema: Columns that define the physical type schema of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type orc_schema: object
        :param orc_parameters: Parameters for dataset.
        :type orc_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param orc_annotations: List of tags that can be used for describing the Dataset.
        :type orc_annotations: list[object]
        :param orc_folder: The folder that this Dataset is in. If not specified, Dataset will appear at
         the root level.
        :type orc_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param orc_location: The location of the ORC data storage.
        :type orc_location: ~azure.mgmt.datafactory.models.DatasetLocation
        :param orc_orc_compression_codec:
        :type orc_orc_compression_codec: str or ~azure.mgmt.datafactory.models.OrcCompressionCodec
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=orc_type, description=orc_description, structure=orc_structure, schema=orc_schema, linked_service_name=orc_linked_service_name, parameters=orc_parameters, annotations=orc_annotations, folder=orc_folder, location=orc_location, orc_compression_codec=orc_orc_compression_codec)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_orc.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_orc.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_parquet(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        parquet_type: str,
        parquet_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        parquet_description: Optional[str] = None,
        parquet_structure: Optional[object] = None,
        parquet_schema: Optional[object] = None,
        parquet_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        parquet_annotations: Optional[List[object]] = None,
        parquet_folder: Optional["models.DatasetFolder"] = None,
        parquet_location: Optional["models.DatasetLocation"] = None,
        parquet_compression_codec: Optional[Union[str, "models.CompressionCodec"]] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param parquet_type: Type of dataset.
        :type parquet_type: str
        :param parquet_linked_service_name: Linked service reference.
        :type parquet_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param parquet_description: Dataset description.
        :type parquet_description: str
        :param parquet_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type parquet_structure: object
        :param parquet_schema: Columns that define the physical type schema of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type parquet_schema: object
        :param parquet_parameters: Parameters for dataset.
        :type parquet_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param parquet_annotations: List of tags that can be used for describing the Dataset.
        :type parquet_annotations: list[object]
        :param parquet_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type parquet_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param parquet_location: The location of the parquet storage.
        :type parquet_location: ~azure.mgmt.datafactory.models.DatasetLocation
        :param parquet_compression_codec:
        :type parquet_compression_codec: str or ~azure.mgmt.datafactory.models.CompressionCodec
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=parquet_type, description=parquet_description, structure=parquet_structure, schema=parquet_schema, linked_service_name=parquet_linked_service_name, parameters=parquet_parameters, annotations=parquet_annotations, folder=parquet_folder, location=parquet_location, compression_codec=parquet_compression_codec)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_parquet.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_parquet.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_paypalobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        paypalobject_type: str,
        paypalobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        paypalobject_description: Optional[str] = None,
        paypalobject_structure: Optional[object] = None,
        paypalobject_schema: Optional[object] = None,
        paypalobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        paypalobject_annotations: Optional[List[object]] = None,
        paypalobject_folder: Optional["models.DatasetFolder"] = None,
        paypalobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param paypalobject_type: Type of dataset.
        :type paypalobject_type: str
        :param paypalobject_linked_service_name: Linked service reference.
        :type paypalobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param paypalobject_description: Dataset description.
        :type paypalobject_description: str
        :param paypalobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type paypalobject_structure: object
        :param paypalobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type paypalobject_schema: object
        :param paypalobject_parameters: Parameters for dataset.
        :type paypalobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param paypalobject_annotations: List of tags that can be used for describing the Dataset.
        :type paypalobject_annotations: list[object]
        :param paypalobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type paypalobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param paypalobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type paypalobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=paypalobject_type, description=paypalobject_description, structure=paypalobject_structure, schema=paypalobject_schema, linked_service_name=paypalobject_linked_service_name, parameters=paypalobject_parameters, annotations=paypalobject_annotations, folder=paypalobject_folder, table_name=paypalobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_paypalobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_paypalobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_phoenixobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        phoenixobject_type: str,
        phoenixobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        phoenixobject_description: Optional[str] = None,
        phoenixobject_structure: Optional[object] = None,
        phoenixobject_schema: Optional[object] = None,
        phoenixobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        phoenixobject_annotations: Optional[List[object]] = None,
        phoenixobject_folder: Optional["models.DatasetFolder"] = None,
        phoenixobject_table_name: Optional[object] = None,
        phoenixobject_table: Optional[object] = None,
        phoenixobject_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param phoenixobject_type: Type of dataset.
        :type phoenixobject_type: str
        :param phoenixobject_linked_service_name: Linked service reference.
        :type phoenixobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param phoenixobject_description: Dataset description.
        :type phoenixobject_description: str
        :param phoenixobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type phoenixobject_structure: object
        :param phoenixobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type phoenixobject_schema: object
        :param phoenixobject_parameters: Parameters for dataset.
        :type phoenixobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param phoenixobject_annotations: List of tags that can be used for describing the Dataset.
        :type phoenixobject_annotations: list[object]
        :param phoenixobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type phoenixobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param phoenixobject_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type phoenixobject_table_name: object
        :param phoenixobject_table: The table name of the Phoenix. Type: string (or Expression with
         resultType string).
        :type phoenixobject_table: object
        :param phoenixobject_schema_type_properties_schema: The schema name of the Phoenix. Type:
         string (or Expression with resultType string).
        :type phoenixobject_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=phoenixobject_type, description=phoenixobject_description, structure=phoenixobject_structure, schema=phoenixobject_schema, linked_service_name=phoenixobject_linked_service_name, parameters=phoenixobject_parameters, annotations=phoenixobject_annotations, folder=phoenixobject_folder, table_name=phoenixobject_table_name, table=phoenixobject_table, schema_type_properties_schema=phoenixobject_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_phoenixobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_phoenixobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_postgresqltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        postgresqltable_type: str,
        postgresqltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        postgresqltable_description: Optional[str] = None,
        postgresqltable_structure: Optional[object] = None,
        postgresqltable_schema: Optional[object] = None,
        postgresqltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        postgresqltable_annotations: Optional[List[object]] = None,
        postgresqltable_folder: Optional["models.DatasetFolder"] = None,
        postgresqltable_table_name: Optional[object] = None,
        postgresqltable_table: Optional[object] = None,
        postgresqltable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param postgresqltable_type: Type of dataset.
        :type postgresqltable_type: str
        :param postgresqltable_linked_service_name: Linked service reference.
        :type postgresqltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param postgresqltable_description: Dataset description.
        :type postgresqltable_description: str
        :param postgresqltable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type postgresqltable_structure: object
        :param postgresqltable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type postgresqltable_schema: object
        :param postgresqltable_parameters: Parameters for dataset.
        :type postgresqltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param postgresqltable_annotations: List of tags that can be used for describing the Dataset.
        :type postgresqltable_annotations: list[object]
        :param postgresqltable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type postgresqltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param postgresqltable_table_name: This property will be retired. Please consider using schema
         + table properties instead.
        :type postgresqltable_table_name: object
        :param postgresqltable_table: The PostgreSQL table name. Type: string (or Expression with
         resultType string).
        :type postgresqltable_table: object
        :param postgresqltable_schema_type_properties_schema: The PostgreSQL schema name. Type: string
         (or Expression with resultType string).
        :type postgresqltable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=postgresqltable_type, description=postgresqltable_description, structure=postgresqltable_structure, schema=postgresqltable_schema, linked_service_name=postgresqltable_linked_service_name, parameters=postgresqltable_parameters, annotations=postgresqltable_annotations, folder=postgresqltable_folder, table_name=postgresqltable_table_name, table=postgresqltable_table, schema_type_properties_schema=postgresqltable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_postgresqltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_postgresqltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_prestoobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        prestoobject_type: str,
        prestoobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        prestoobject_description: Optional[str] = None,
        prestoobject_structure: Optional[object] = None,
        prestoobject_schema: Optional[object] = None,
        prestoobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        prestoobject_annotations: Optional[List[object]] = None,
        prestoobject_folder: Optional["models.DatasetFolder"] = None,
        prestoobject_table_name: Optional[object] = None,
        prestoobject_table: Optional[object] = None,
        prestoobject_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param prestoobject_type: Type of dataset.
        :type prestoobject_type: str
        :param prestoobject_linked_service_name: Linked service reference.
        :type prestoobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param prestoobject_description: Dataset description.
        :type prestoobject_description: str
        :param prestoobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type prestoobject_structure: object
        :param prestoobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type prestoobject_schema: object
        :param prestoobject_parameters: Parameters for dataset.
        :type prestoobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param prestoobject_annotations: List of tags that can be used for describing the Dataset.
        :type prestoobject_annotations: list[object]
        :param prestoobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type prestoobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param prestoobject_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type prestoobject_table_name: object
        :param prestoobject_table: The table name of the Presto. Type: string (or Expression with
         resultType string).
        :type prestoobject_table: object
        :param prestoobject_schema_type_properties_schema: The schema name of the Presto. Type: string
         (or Expression with resultType string).
        :type prestoobject_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=prestoobject_type, description=prestoobject_description, structure=prestoobject_structure, schema=prestoobject_schema, linked_service_name=prestoobject_linked_service_name, parameters=prestoobject_parameters, annotations=prestoobject_annotations, folder=prestoobject_folder, table_name=prestoobject_table_name, table=prestoobject_table, schema_type_properties_schema=prestoobject_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_prestoobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_prestoobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_quickbooksobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        quickbooksobject_type: str,
        quickbooksobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        quickbooksobject_description: Optional[str] = None,
        quickbooksobject_structure: Optional[object] = None,
        quickbooksobject_schema: Optional[object] = None,
        quickbooksobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        quickbooksobject_annotations: Optional[List[object]] = None,
        quickbooksobject_folder: Optional["models.DatasetFolder"] = None,
        quickbooksobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param quickbooksobject_type: Type of dataset.
        :type quickbooksobject_type: str
        :param quickbooksobject_linked_service_name: Linked service reference.
        :type quickbooksobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param quickbooksobject_description: Dataset description.
        :type quickbooksobject_description: str
        :param quickbooksobject_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type quickbooksobject_structure: object
        :param quickbooksobject_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type quickbooksobject_schema: object
        :param quickbooksobject_parameters: Parameters for dataset.
        :type quickbooksobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param quickbooksobject_annotations: List of tags that can be used for describing the Dataset.
        :type quickbooksobject_annotations: list[object]
        :param quickbooksobject_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type quickbooksobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param quickbooksobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type quickbooksobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=quickbooksobject_type, description=quickbooksobject_description, structure=quickbooksobject_structure, schema=quickbooksobject_schema, linked_service_name=quickbooksobject_linked_service_name, parameters=quickbooksobject_parameters, annotations=quickbooksobject_annotations, folder=quickbooksobject_folder, table_name=quickbooksobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_quickbooksobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_quickbooksobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_relationaltable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        relationaltable_type: str,
        relationaltable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        relationaltable_description: Optional[str] = None,
        relationaltable_structure: Optional[object] = None,
        relationaltable_schema: Optional[object] = None,
        relationaltable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        relationaltable_annotations: Optional[List[object]] = None,
        relationaltable_folder: Optional["models.DatasetFolder"] = None,
        relationaltable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param relationaltable_type: Type of dataset.
        :type relationaltable_type: str
        :param relationaltable_linked_service_name: Linked service reference.
        :type relationaltable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param relationaltable_description: Dataset description.
        :type relationaltable_description: str
        :param relationaltable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type relationaltable_structure: object
        :param relationaltable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type relationaltable_schema: object
        :param relationaltable_parameters: Parameters for dataset.
        :type relationaltable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param relationaltable_annotations: List of tags that can be used for describing the Dataset.
        :type relationaltable_annotations: list[object]
        :param relationaltable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type relationaltable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param relationaltable_table_name: The relational table name. Type: string (or Expression with
         resultType string).
        :type relationaltable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=relationaltable_type, description=relationaltable_description, structure=relationaltable_structure, schema=relationaltable_schema, linked_service_name=relationaltable_linked_service_name, parameters=relationaltable_parameters, annotations=relationaltable_annotations, folder=relationaltable_folder, table_name=relationaltable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_relationaltable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_relationaltable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_responsysobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        responsysobject_type: str,
        responsysobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        responsysobject_description: Optional[str] = None,
        responsysobject_structure: Optional[object] = None,
        responsysobject_schema: Optional[object] = None,
        responsysobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        responsysobject_annotations: Optional[List[object]] = None,
        responsysobject_folder: Optional["models.DatasetFolder"] = None,
        responsysobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param responsysobject_type: Type of dataset.
        :type responsysobject_type: str
        :param responsysobject_linked_service_name: Linked service reference.
        :type responsysobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param responsysobject_description: Dataset description.
        :type responsysobject_description: str
        :param responsysobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type responsysobject_structure: object
        :param responsysobject_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type responsysobject_schema: object
        :param responsysobject_parameters: Parameters for dataset.
        :type responsysobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param responsysobject_annotations: List of tags that can be used for describing the Dataset.
        :type responsysobject_annotations: list[object]
        :param responsysobject_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type responsysobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param responsysobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type responsysobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=responsysobject_type, description=responsysobject_description, structure=responsysobject_structure, schema=responsysobject_schema, linked_service_name=responsysobject_linked_service_name, parameters=responsysobject_parameters, annotations=responsysobject_annotations, folder=responsysobject_folder, table_name=responsysobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_responsysobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_responsysobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_restresource(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        restresource_type: str,
        restresource_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        restresource_description: Optional[str] = None,
        restresource_structure: Optional[object] = None,
        restresource_schema: Optional[object] = None,
        restresource_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        restresource_annotations: Optional[List[object]] = None,
        restresource_folder: Optional["models.DatasetFolder"] = None,
        restresource_relative_url: Optional[object] = None,
        restresource_request_method: Optional[object] = None,
        restresource_request_body: Optional[object] = None,
        restresource_additional_headers: Optional[object] = None,
        restresource_pagination_rules: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param restresource_type: Type of dataset.
        :type restresource_type: str
        :param restresource_linked_service_name: Linked service reference.
        :type restresource_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param restresource_description: Dataset description.
        :type restresource_description: str
        :param restresource_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type restresource_structure: object
        :param restresource_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type restresource_schema: object
        :param restresource_parameters: Parameters for dataset.
        :type restresource_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param restresource_annotations: List of tags that can be used for describing the Dataset.
        :type restresource_annotations: list[object]
        :param restresource_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type restresource_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param restresource_relative_url: The relative URL to the resource that the RESTful API
         provides. Type: string (or Expression with resultType string).
        :type restresource_relative_url: object
        :param restresource_request_method: The HTTP method used to call the RESTful API. The default
         is GET. Type: string (or Expression with resultType string).
        :type restresource_request_method: object
        :param restresource_request_body: The HTTP request body to the RESTful API if requestMethod is
         POST. Type: string (or Expression with resultType string).
        :type restresource_request_body: object
        :param restresource_additional_headers: The additional HTTP headers in the request to the
         RESTful API. Type: string (or Expression with resultType string).
        :type restresource_additional_headers: object
        :param restresource_pagination_rules: The pagination rules to compose next page requests. Type:
         string (or Expression with resultType string).
        :type restresource_pagination_rules: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=restresource_type, description=restresource_description, structure=restresource_structure, schema=restresource_schema, linked_service_name=restresource_linked_service_name, parameters=restresource_parameters, annotations=restresource_annotations, folder=restresource_folder, relative_url=restresource_relative_url, request_method=restresource_request_method, request_body=restresource_request_body, additional_headers=restresource_additional_headers, pagination_rules=restresource_pagination_rules)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_restresource.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_restresource.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_salesforcemarketingcloudobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        salesforcemarketingcloudobject_type: str,
        salesforcemarketingcloudobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        salesforcemarketingcloudobject_description: Optional[str] = None,
        salesforcemarketingcloudobject_structure: Optional[object] = None,
        salesforcemarketingcloudobject_schema: Optional[object] = None,
        salesforcemarketingcloudobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        salesforcemarketingcloudobject_annotations: Optional[List[object]] = None,
        salesforcemarketingcloudobject_folder: Optional["models.DatasetFolder"] = None,
        salesforcemarketingcloudobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param salesforcemarketingcloudobject_type: Type of dataset.
        :type salesforcemarketingcloudobject_type: str
        :param salesforcemarketingcloudobject_linked_service_name: Linked service reference.
        :type salesforcemarketingcloudobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param salesforcemarketingcloudobject_description: Dataset description.
        :type salesforcemarketingcloudobject_description: str
        :param salesforcemarketingcloudobject_structure: Columns that define the structure of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type salesforcemarketingcloudobject_structure: object
        :param salesforcemarketingcloudobject_schema: Columns that define the physical type schema of
         the dataset. Type: array (or Expression with resultType array), itemType:
         DatasetSchemaDataElement.
        :type salesforcemarketingcloudobject_schema: object
        :param salesforcemarketingcloudobject_parameters: Parameters for dataset.
        :type salesforcemarketingcloudobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param salesforcemarketingcloudobject_annotations: List of tags that can be used for describing
         the Dataset.
        :type salesforcemarketingcloudobject_annotations: list[object]
        :param salesforcemarketingcloudobject_folder: The folder that this Dataset is in. If not
         specified, Dataset will appear at the root level.
        :type salesforcemarketingcloudobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param salesforcemarketingcloudobject_table_name: The table name. Type: string (or Expression
         with resultType string).
        :type salesforcemarketingcloudobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=salesforcemarketingcloudobject_type, description=salesforcemarketingcloudobject_description, structure=salesforcemarketingcloudobject_structure, schema=salesforcemarketingcloudobject_schema, linked_service_name=salesforcemarketingcloudobject_linked_service_name, parameters=salesforcemarketingcloudobject_parameters, annotations=salesforcemarketingcloudobject_annotations, folder=salesforcemarketingcloudobject_folder, table_name=salesforcemarketingcloudobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_salesforcemarketingcloudobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_salesforcemarketingcloudobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_salesforceobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        salesforceobject_type: str,
        salesforceobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        salesforceobject_description: Optional[str] = None,
        salesforceobject_structure: Optional[object] = None,
        salesforceobject_schema: Optional[object] = None,
        salesforceobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        salesforceobject_annotations: Optional[List[object]] = None,
        salesforceobject_folder: Optional["models.DatasetFolder"] = None,
        salesforceobject_object_api_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param salesforceobject_type: Type of dataset.
        :type salesforceobject_type: str
        :param salesforceobject_linked_service_name: Linked service reference.
        :type salesforceobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param salesforceobject_description: Dataset description.
        :type salesforceobject_description: str
        :param salesforceobject_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type salesforceobject_structure: object
        :param salesforceobject_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type salesforceobject_schema: object
        :param salesforceobject_parameters: Parameters for dataset.
        :type salesforceobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param salesforceobject_annotations: List of tags that can be used for describing the Dataset.
        :type salesforceobject_annotations: list[object]
        :param salesforceobject_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type salesforceobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param salesforceobject_object_api_name: The Salesforce object API name. Type: string (or
         Expression with resultType string).
        :type salesforceobject_object_api_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=salesforceobject_type, description=salesforceobject_description, structure=salesforceobject_structure, schema=salesforceobject_schema, linked_service_name=salesforceobject_linked_service_name, parameters=salesforceobject_parameters, annotations=salesforceobject_annotations, folder=salesforceobject_folder, object_api_name=salesforceobject_object_api_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_salesforceobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_salesforceobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_salesforceservicecloudobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        salesforceservicecloudobject_type: str,
        salesforceservicecloudobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        salesforceservicecloudobject_description: Optional[str] = None,
        salesforceservicecloudobject_structure: Optional[object] = None,
        salesforceservicecloudobject_schema: Optional[object] = None,
        salesforceservicecloudobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        salesforceservicecloudobject_annotations: Optional[List[object]] = None,
        salesforceservicecloudobject_folder: Optional["models.DatasetFolder"] = None,
        salesforceservicecloudobject_object_api_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param salesforceservicecloudobject_type: Type of dataset.
        :type salesforceservicecloudobject_type: str
        :param salesforceservicecloudobject_linked_service_name: Linked service reference.
        :type salesforceservicecloudobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param salesforceservicecloudobject_description: Dataset description.
        :type salesforceservicecloudobject_description: str
        :param salesforceservicecloudobject_structure: Columns that define the structure of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type salesforceservicecloudobject_structure: object
        :param salesforceservicecloudobject_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type salesforceservicecloudobject_schema: object
        :param salesforceservicecloudobject_parameters: Parameters for dataset.
        :type salesforceservicecloudobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param salesforceservicecloudobject_annotations: List of tags that can be used for describing
         the Dataset.
        :type salesforceservicecloudobject_annotations: list[object]
        :param salesforceservicecloudobject_folder: The folder that this Dataset is in. If not
         specified, Dataset will appear at the root level.
        :type salesforceservicecloudobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param salesforceservicecloudobject_object_api_name: The Salesforce Service Cloud object API
         name. Type: string (or Expression with resultType string).
        :type salesforceservicecloudobject_object_api_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=salesforceservicecloudobject_type, description=salesforceservicecloudobject_description, structure=salesforceservicecloudobject_structure, schema=salesforceservicecloudobject_schema, linked_service_name=salesforceservicecloudobject_linked_service_name, parameters=salesforceservicecloudobject_parameters, annotations=salesforceservicecloudobject_annotations, folder=salesforceservicecloudobject_folder, object_api_name=salesforceservicecloudobject_object_api_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_salesforceservicecloudobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_salesforceservicecloudobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sapbwcube(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sapbwcube_type: str,
        sapbwcube_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        sapbwcube_description: Optional[str] = None,
        sapbwcube_structure: Optional[object] = None,
        sapbwcube_schema: Optional[object] = None,
        sapbwcube_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sapbwcube_annotations: Optional[List[object]] = None,
        sapbwcube_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sapbwcube_type: Type of dataset.
        :type sapbwcube_type: str
        :param sapbwcube_linked_service_name: Linked service reference.
        :type sapbwcube_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sapbwcube_description: Dataset description.
        :type sapbwcube_description: str
        :param sapbwcube_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type sapbwcube_structure: object
        :param sapbwcube_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sapbwcube_schema: object
        :param sapbwcube_parameters: Parameters for dataset.
        :type sapbwcube_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sapbwcube_annotations: List of tags that can be used for describing the Dataset.
        :type sapbwcube_annotations: list[object]
        :param sapbwcube_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type sapbwcube_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sapbwcube_type, description=sapbwcube_description, structure=sapbwcube_structure, schema=sapbwcube_schema, linked_service_name=sapbwcube_linked_service_name, parameters=sapbwcube_parameters, annotations=sapbwcube_annotations, folder=sapbwcube_folder)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sapbwcube.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sapbwcube.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sapcloudforcustomerresource(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sapcloudforcustomerresource_type: str,
        sapcloudforcustomerresource_linked_service_name: "models.LinkedServiceReference",
        sapcloudforcustomerresource_path: object,
        if_match: Optional[str] = None,
        sapcloudforcustomerresource_description: Optional[str] = None,
        sapcloudforcustomerresource_structure: Optional[object] = None,
        sapcloudforcustomerresource_schema: Optional[object] = None,
        sapcloudforcustomerresource_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sapcloudforcustomerresource_annotations: Optional[List[object]] = None,
        sapcloudforcustomerresource_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sapcloudforcustomerresource_type: Type of dataset.
        :type sapcloudforcustomerresource_type: str
        :param sapcloudforcustomerresource_linked_service_name: Linked service reference.
        :type sapcloudforcustomerresource_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param sapcloudforcustomerresource_path: The path of the SAP Cloud for Customer OData entity.
         Type: string (or Expression with resultType string).
        :type sapcloudforcustomerresource_path: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sapcloudforcustomerresource_description: Dataset description.
        :type sapcloudforcustomerresource_description: str
        :param sapcloudforcustomerresource_structure: Columns that define the structure of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetDataElement.
        :type sapcloudforcustomerresource_structure: object
        :param sapcloudforcustomerresource_schema: Columns that define the physical type schema of the
         dataset. Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sapcloudforcustomerresource_schema: object
        :param sapcloudforcustomerresource_parameters: Parameters for dataset.
        :type sapcloudforcustomerresource_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sapcloudforcustomerresource_annotations: List of tags that can be used for describing
         the Dataset.
        :type sapcloudforcustomerresource_annotations: list[object]
        :param sapcloudforcustomerresource_folder: The folder that this Dataset is in. If not
         specified, Dataset will appear at the root level.
        :type sapcloudforcustomerresource_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sapcloudforcustomerresource_type, description=sapcloudforcustomerresource_description, structure=sapcloudforcustomerresource_structure, schema=sapcloudforcustomerresource_schema, linked_service_name=sapcloudforcustomerresource_linked_service_name, parameters=sapcloudforcustomerresource_parameters, annotations=sapcloudforcustomerresource_annotations, folder=sapcloudforcustomerresource_folder, path=sapcloudforcustomerresource_path)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sapcloudforcustomerresource.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sapcloudforcustomerresource.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sapeccresource(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sapeccresource_type: str,
        sapeccresource_linked_service_name: "models.LinkedServiceReference",
        sapeccresource_path: object,
        if_match: Optional[str] = None,
        sapeccresource_description: Optional[str] = None,
        sapeccresource_structure: Optional[object] = None,
        sapeccresource_schema: Optional[object] = None,
        sapeccresource_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sapeccresource_annotations: Optional[List[object]] = None,
        sapeccresource_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sapeccresource_type: Type of dataset.
        :type sapeccresource_type: str
        :param sapeccresource_linked_service_name: Linked service reference.
        :type sapeccresource_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param sapeccresource_path: The path of the SAP ECC OData entity. Type: string (or Expression
         with resultType string).
        :type sapeccresource_path: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sapeccresource_description: Dataset description.
        :type sapeccresource_description: str
        :param sapeccresource_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type sapeccresource_structure: object
        :param sapeccresource_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sapeccresource_schema: object
        :param sapeccresource_parameters: Parameters for dataset.
        :type sapeccresource_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sapeccresource_annotations: List of tags that can be used for describing the Dataset.
        :type sapeccresource_annotations: list[object]
        :param sapeccresource_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type sapeccresource_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sapeccresource_type, description=sapeccresource_description, structure=sapeccresource_structure, schema=sapeccresource_schema, linked_service_name=sapeccresource_linked_service_name, parameters=sapeccresource_parameters, annotations=sapeccresource_annotations, folder=sapeccresource_folder, path=sapeccresource_path)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sapeccresource.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sapeccresource.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_saphanatable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        saphanatable_type: str,
        saphanatable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        saphanatable_description: Optional[str] = None,
        saphanatable_structure: Optional[object] = None,
        saphanatable_schema: Optional[object] = None,
        saphanatable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        saphanatable_annotations: Optional[List[object]] = None,
        saphanatable_folder: Optional["models.DatasetFolder"] = None,
        saphanatable_schema_type_properties_schema: Optional[object] = None,
        saphanatable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param saphanatable_type: Type of dataset.
        :type saphanatable_type: str
        :param saphanatable_linked_service_name: Linked service reference.
        :type saphanatable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param saphanatable_description: Dataset description.
        :type saphanatable_description: str
        :param saphanatable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type saphanatable_structure: object
        :param saphanatable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type saphanatable_schema: object
        :param saphanatable_parameters: Parameters for dataset.
        :type saphanatable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param saphanatable_annotations: List of tags that can be used for describing the Dataset.
        :type saphanatable_annotations: list[object]
        :param saphanatable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type saphanatable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param saphanatable_schema_type_properties_schema: The schema name of SAP HANA. Type: string
         (or Expression with resultType string).
        :type saphanatable_schema_type_properties_schema: object
        :param saphanatable_table: The table name of SAP HANA. Type: string (or Expression with
         resultType string).
        :type saphanatable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=saphanatable_type, description=saphanatable_description, structure=saphanatable_structure, schema=saphanatable_schema, linked_service_name=saphanatable_linked_service_name, parameters=saphanatable_parameters, annotations=saphanatable_annotations, folder=saphanatable_folder, schema_type_properties_schema=saphanatable_schema_type_properties_schema, table=saphanatable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_saphanatable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_saphanatable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sapopenhubtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sapopenhubtable_type: str,
        sapopenhubtable_linked_service_name: "models.LinkedServiceReference",
        sapopenhubtable_open_hub_destination_name: object,
        if_match: Optional[str] = None,
        sapopenhubtable_description: Optional[str] = None,
        sapopenhubtable_structure: Optional[object] = None,
        sapopenhubtable_schema: Optional[object] = None,
        sapopenhubtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sapopenhubtable_annotations: Optional[List[object]] = None,
        sapopenhubtable_folder: Optional["models.DatasetFolder"] = None,
        sapopenhubtable_exclude_last_request: Optional[object] = None,
        sapopenhubtable_base_request_id: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sapopenhubtable_type: Type of dataset.
        :type sapopenhubtable_type: str
        :param sapopenhubtable_linked_service_name: Linked service reference.
        :type sapopenhubtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param sapopenhubtable_open_hub_destination_name: The name of the Open Hub Destination with
         destination type as Database Table. Type: string (or Expression with resultType string).
        :type sapopenhubtable_open_hub_destination_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sapopenhubtable_description: Dataset description.
        :type sapopenhubtable_description: str
        :param sapopenhubtable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type sapopenhubtable_structure: object
        :param sapopenhubtable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sapopenhubtable_schema: object
        :param sapopenhubtable_parameters: Parameters for dataset.
        :type sapopenhubtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sapopenhubtable_annotations: List of tags that can be used for describing the Dataset.
        :type sapopenhubtable_annotations: list[object]
        :param sapopenhubtable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type sapopenhubtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param sapopenhubtable_exclude_last_request: Whether to exclude the records of the last
         request. The default value is true. Type: boolean (or Expression with resultType boolean).
        :type sapopenhubtable_exclude_last_request: object
        :param sapopenhubtable_base_request_id: The ID of request for delta loading. Once it is set,
         only data with requestId larger than the value of this property will be retrieved. The default
         value is 0. Type: integer (or Expression with resultType integer ).
        :type sapopenhubtable_base_request_id: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sapopenhubtable_type, description=sapopenhubtable_description, structure=sapopenhubtable_structure, schema=sapopenhubtable_schema, linked_service_name=sapopenhubtable_linked_service_name, parameters=sapopenhubtable_parameters, annotations=sapopenhubtable_annotations, folder=sapopenhubtable_folder, open_hub_destination_name=sapopenhubtable_open_hub_destination_name, exclude_last_request=sapopenhubtable_exclude_last_request, base_request_id=sapopenhubtable_base_request_id)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sapopenhubtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sapopenhubtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_saptableresource(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        saptableresource_type: str,
        saptableresource_linked_service_name: "models.LinkedServiceReference",
        saptableresource_table_name: object,
        if_match: Optional[str] = None,
        saptableresource_description: Optional[str] = None,
        saptableresource_structure: Optional[object] = None,
        saptableresource_schema: Optional[object] = None,
        saptableresource_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        saptableresource_annotations: Optional[List[object]] = None,
        saptableresource_folder: Optional["models.DatasetFolder"] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param saptableresource_type: Type of dataset.
        :type saptableresource_type: str
        :param saptableresource_linked_service_name: Linked service reference.
        :type saptableresource_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param saptableresource_table_name: The name of the SAP Table. Type: string (or Expression with
         resultType string).
        :type saptableresource_table_name: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param saptableresource_description: Dataset description.
        :type saptableresource_description: str
        :param saptableresource_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type saptableresource_structure: object
        :param saptableresource_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type saptableresource_schema: object
        :param saptableresource_parameters: Parameters for dataset.
        :type saptableresource_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param saptableresource_annotations: List of tags that can be used for describing the Dataset.
        :type saptableresource_annotations: list[object]
        :param saptableresource_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type saptableresource_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=saptableresource_type, description=saptableresource_description, structure=saptableresource_structure, schema=saptableresource_schema, linked_service_name=saptableresource_linked_service_name, parameters=saptableresource_parameters, annotations=saptableresource_annotations, folder=saptableresource_folder, table_name=saptableresource_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_saptableresource.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_saptableresource.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_servicenowobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        servicenowobject_type: str,
        servicenowobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        servicenowobject_description: Optional[str] = None,
        servicenowobject_structure: Optional[object] = None,
        servicenowobject_schema: Optional[object] = None,
        servicenowobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        servicenowobject_annotations: Optional[List[object]] = None,
        servicenowobject_folder: Optional["models.DatasetFolder"] = None,
        servicenowobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param servicenowobject_type: Type of dataset.
        :type servicenowobject_type: str
        :param servicenowobject_linked_service_name: Linked service reference.
        :type servicenowobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param servicenowobject_description: Dataset description.
        :type servicenowobject_description: str
        :param servicenowobject_structure: Columns that define the structure of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetDataElement.
        :type servicenowobject_structure: object
        :param servicenowobject_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type servicenowobject_schema: object
        :param servicenowobject_parameters: Parameters for dataset.
        :type servicenowobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param servicenowobject_annotations: List of tags that can be used for describing the Dataset.
        :type servicenowobject_annotations: list[object]
        :param servicenowobject_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type servicenowobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param servicenowobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type servicenowobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=servicenowobject_type, description=servicenowobject_description, structure=servicenowobject_structure, schema=servicenowobject_schema, linked_service_name=servicenowobject_linked_service_name, parameters=servicenowobject_parameters, annotations=servicenowobject_annotations, folder=servicenowobject_folder, table_name=servicenowobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_servicenowobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_servicenowobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_shopifyobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        shopifyobject_type: str,
        shopifyobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        shopifyobject_description: Optional[str] = None,
        shopifyobject_structure: Optional[object] = None,
        shopifyobject_schema: Optional[object] = None,
        shopifyobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        shopifyobject_annotations: Optional[List[object]] = None,
        shopifyobject_folder: Optional["models.DatasetFolder"] = None,
        shopifyobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param shopifyobject_type: Type of dataset.
        :type shopifyobject_type: str
        :param shopifyobject_linked_service_name: Linked service reference.
        :type shopifyobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param shopifyobject_description: Dataset description.
        :type shopifyobject_description: str
        :param shopifyobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type shopifyobject_structure: object
        :param shopifyobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type shopifyobject_schema: object
        :param shopifyobject_parameters: Parameters for dataset.
        :type shopifyobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param shopifyobject_annotations: List of tags that can be used for describing the Dataset.
        :type shopifyobject_annotations: list[object]
        :param shopifyobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type shopifyobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param shopifyobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type shopifyobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=shopifyobject_type, description=shopifyobject_description, structure=shopifyobject_structure, schema=shopifyobject_schema, linked_service_name=shopifyobject_linked_service_name, parameters=shopifyobject_parameters, annotations=shopifyobject_annotations, folder=shopifyobject_folder, table_name=shopifyobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_shopifyobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_shopifyobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_snowflaketable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        snowflaketable_type: str,
        snowflaketable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        snowflaketable_description: Optional[str] = None,
        snowflaketable_structure: Optional[object] = None,
        snowflaketable_schema: Optional[object] = None,
        snowflaketable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        snowflaketable_annotations: Optional[List[object]] = None,
        snowflaketable_folder: Optional["models.DatasetFolder"] = None,
        snowflaketable_schema_type_properties_schema: Optional[object] = None,
        snowflaketable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param snowflaketable_type: Type of dataset.
        :type snowflaketable_type: str
        :param snowflaketable_linked_service_name: Linked service reference.
        :type snowflaketable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param snowflaketable_description: Dataset description.
        :type snowflaketable_description: str
        :param snowflaketable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type snowflaketable_structure: object
        :param snowflaketable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type snowflaketable_schema: object
        :param snowflaketable_parameters: Parameters for dataset.
        :type snowflaketable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param snowflaketable_annotations: List of tags that can be used for describing the Dataset.
        :type snowflaketable_annotations: list[object]
        :param snowflaketable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type snowflaketable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param snowflaketable_schema_type_properties_schema: The schema name of the Snowflake database.
         Type: string (or Expression with resultType string).
        :type snowflaketable_schema_type_properties_schema: object
        :param snowflaketable_table: The table name of the Snowflake database. Type: string (or
         Expression with resultType string).
        :type snowflaketable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=snowflaketable_type, description=snowflaketable_description, structure=snowflaketable_structure, schema=snowflaketable_schema, linked_service_name=snowflaketable_linked_service_name, parameters=snowflaketable_parameters, annotations=snowflaketable_annotations, folder=snowflaketable_folder, schema_type_properties_schema=snowflaketable_schema_type_properties_schema, table=snowflaketable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_snowflaketable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_snowflaketable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sparkobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sparkobject_type: str,
        sparkobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        sparkobject_description: Optional[str] = None,
        sparkobject_structure: Optional[object] = None,
        sparkobject_schema: Optional[object] = None,
        sparkobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sparkobject_annotations: Optional[List[object]] = None,
        sparkobject_folder: Optional["models.DatasetFolder"] = None,
        sparkobject_table_name: Optional[object] = None,
        sparkobject_table: Optional[object] = None,
        sparkobject_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sparkobject_type: Type of dataset.
        :type sparkobject_type: str
        :param sparkobject_linked_service_name: Linked service reference.
        :type sparkobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sparkobject_description: Dataset description.
        :type sparkobject_description: str
        :param sparkobject_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type sparkobject_structure: object
        :param sparkobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sparkobject_schema: object
        :param sparkobject_parameters: Parameters for dataset.
        :type sparkobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sparkobject_annotations: List of tags that can be used for describing the Dataset.
        :type sparkobject_annotations: list[object]
        :param sparkobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type sparkobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param sparkobject_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type sparkobject_table_name: object
        :param sparkobject_table: The table name of the Spark. Type: string (or Expression with
         resultType string).
        :type sparkobject_table: object
        :param sparkobject_schema_type_properties_schema: The schema name of the Spark. Type: string
         (or Expression with resultType string).
        :type sparkobject_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sparkobject_type, description=sparkobject_description, structure=sparkobject_structure, schema=sparkobject_schema, linked_service_name=sparkobject_linked_service_name, parameters=sparkobject_parameters, annotations=sparkobject_annotations, folder=sparkobject_folder, table_name=sparkobject_table_name, table=sparkobject_table, schema_type_properties_schema=sparkobject_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sparkobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sparkobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sqlservertable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sqlservertable_type: str,
        sqlservertable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        sqlservertable_description: Optional[str] = None,
        sqlservertable_structure: Optional[object] = None,
        sqlservertable_schema: Optional[object] = None,
        sqlservertable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sqlservertable_annotations: Optional[List[object]] = None,
        sqlservertable_folder: Optional["models.DatasetFolder"] = None,
        sqlservertable_table_name: Optional[object] = None,
        sqlservertable_schema_type_properties_schema: Optional[object] = None,
        sqlservertable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sqlservertable_type: Type of dataset.
        :type sqlservertable_type: str
        :param sqlservertable_linked_service_name: Linked service reference.
        :type sqlservertable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sqlservertable_description: Dataset description.
        :type sqlservertable_description: str
        :param sqlservertable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type sqlservertable_structure: object
        :param sqlservertable_schema: Columns that define the physical type schema of the dataset.
         Type: array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sqlservertable_schema: object
        :param sqlservertable_parameters: Parameters for dataset.
        :type sqlservertable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sqlservertable_annotations: List of tags that can be used for describing the Dataset.
        :type sqlservertable_annotations: list[object]
        :param sqlservertable_folder: The folder that this Dataset is in. If not specified, Dataset
         will appear at the root level.
        :type sqlservertable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param sqlservertable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type sqlservertable_table_name: object
        :param sqlservertable_schema_type_properties_schema: The schema name of the SQL Server dataset.
         Type: string (or Expression with resultType string).
        :type sqlservertable_schema_type_properties_schema: object
        :param sqlservertable_table: The table name of the SQL Server dataset. Type: string (or
         Expression with resultType string).
        :type sqlservertable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sqlservertable_type, description=sqlservertable_description, structure=sqlservertable_structure, schema=sqlservertable_schema, linked_service_name=sqlservertable_linked_service_name, parameters=sqlservertable_parameters, annotations=sqlservertable_annotations, folder=sqlservertable_folder, table_name=sqlservertable_table_name, schema_type_properties_schema=sqlservertable_schema_type_properties_schema, table=sqlservertable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sqlservertable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sqlservertable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_squareobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        squareobject_type: str,
        squareobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        squareobject_description: Optional[str] = None,
        squareobject_structure: Optional[object] = None,
        squareobject_schema: Optional[object] = None,
        squareobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        squareobject_annotations: Optional[List[object]] = None,
        squareobject_folder: Optional["models.DatasetFolder"] = None,
        squareobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param squareobject_type: Type of dataset.
        :type squareobject_type: str
        :param squareobject_linked_service_name: Linked service reference.
        :type squareobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param squareobject_description: Dataset description.
        :type squareobject_description: str
        :param squareobject_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type squareobject_structure: object
        :param squareobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type squareobject_schema: object
        :param squareobject_parameters: Parameters for dataset.
        :type squareobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param squareobject_annotations: List of tags that can be used for describing the Dataset.
        :type squareobject_annotations: list[object]
        :param squareobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type squareobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param squareobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type squareobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=squareobject_type, description=squareobject_description, structure=squareobject_structure, schema=squareobject_schema, linked_service_name=squareobject_linked_service_name, parameters=squareobject_parameters, annotations=squareobject_annotations, folder=squareobject_folder, table_name=squareobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_squareobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_squareobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_sybasetable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        sybasetable_type: str,
        sybasetable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        sybasetable_description: Optional[str] = None,
        sybasetable_structure: Optional[object] = None,
        sybasetable_schema: Optional[object] = None,
        sybasetable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        sybasetable_annotations: Optional[List[object]] = None,
        sybasetable_folder: Optional["models.DatasetFolder"] = None,
        sybasetable_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param sybasetable_type: Type of dataset.
        :type sybasetable_type: str
        :param sybasetable_linked_service_name: Linked service reference.
        :type sybasetable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param sybasetable_description: Dataset description.
        :type sybasetable_description: str
        :param sybasetable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type sybasetable_structure: object
        :param sybasetable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type sybasetable_schema: object
        :param sybasetable_parameters: Parameters for dataset.
        :type sybasetable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param sybasetable_annotations: List of tags that can be used for describing the Dataset.
        :type sybasetable_annotations: list[object]
        :param sybasetable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type sybasetable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param sybasetable_table_name: The Sybase table name. Type: string (or Expression with
         resultType string).
        :type sybasetable_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=sybasetable_type, description=sybasetable_description, structure=sybasetable_structure, schema=sybasetable_schema, linked_service_name=sybasetable_linked_service_name, parameters=sybasetable_parameters, annotations=sybasetable_annotations, folder=sybasetable_folder, table_name=sybasetable_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_sybasetable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_sybasetable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_teradatatable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        teradatatable_type: str,
        teradatatable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        teradatatable_description: Optional[str] = None,
        teradatatable_structure: Optional[object] = None,
        teradatatable_schema: Optional[object] = None,
        teradatatable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        teradatatable_annotations: Optional[List[object]] = None,
        teradatatable_folder: Optional["models.DatasetFolder"] = None,
        teradatatable_database: Optional[object] = None,
        teradatatable_table: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param teradatatable_type: Type of dataset.
        :type teradatatable_type: str
        :param teradatatable_linked_service_name: Linked service reference.
        :type teradatatable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param teradatatable_description: Dataset description.
        :type teradatatable_description: str
        :param teradatatable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type teradatatable_structure: object
        :param teradatatable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type teradatatable_schema: object
        :param teradatatable_parameters: Parameters for dataset.
        :type teradatatable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param teradatatable_annotations: List of tags that can be used for describing the Dataset.
        :type teradatatable_annotations: list[object]
        :param teradatatable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type teradatatable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param teradatatable_database: The database name of Teradata. Type: string (or Expression with
         resultType string).
        :type teradatatable_database: object
        :param teradatatable_table: The table name of Teradata. Type: string (or Expression with
         resultType string).
        :type teradatatable_table: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=teradatatable_type, description=teradatatable_description, structure=teradatatable_structure, schema=teradatatable_schema, linked_service_name=teradatatable_linked_service_name, parameters=teradatatable_parameters, annotations=teradatatable_annotations, folder=teradatatable_folder, database=teradatatable_database, table=teradatatable_table)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_teradatatable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_teradatatable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_verticatable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        verticatable_type: str,
        verticatable_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        verticatable_description: Optional[str] = None,
        verticatable_structure: Optional[object] = None,
        verticatable_schema: Optional[object] = None,
        verticatable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        verticatable_annotations: Optional[List[object]] = None,
        verticatable_folder: Optional["models.DatasetFolder"] = None,
        verticatable_table_name: Optional[object] = None,
        verticatable_table: Optional[object] = None,
        verticatable_schema_type_properties_schema: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param verticatable_type: Type of dataset.
        :type verticatable_type: str
        :param verticatable_linked_service_name: Linked service reference.
        :type verticatable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param verticatable_description: Dataset description.
        :type verticatable_description: str
        :param verticatable_structure: Columns that define the structure of the dataset. Type: array
         (or Expression with resultType array), itemType: DatasetDataElement.
        :type verticatable_structure: object
        :param verticatable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type verticatable_schema: object
        :param verticatable_parameters: Parameters for dataset.
        :type verticatable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param verticatable_annotations: List of tags that can be used for describing the Dataset.
        :type verticatable_annotations: list[object]
        :param verticatable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type verticatable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param verticatable_table_name: This property will be retired. Please consider using schema +
         table properties instead.
        :type verticatable_table_name: object
        :param verticatable_table: The table name of the Vertica. Type: string (or Expression with
         resultType string).
        :type verticatable_table: object
        :param verticatable_schema_type_properties_schema: The schema name of the Vertica. Type: string
         (or Expression with resultType string).
        :type verticatable_schema_type_properties_schema: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=verticatable_type, description=verticatable_description, structure=verticatable_structure, schema=verticatable_schema, linked_service_name=verticatable_linked_service_name, parameters=verticatable_parameters, annotations=verticatable_annotations, folder=verticatable_folder, table_name=verticatable_table_name, table=verticatable_table, schema_type_properties_schema=verticatable_schema_type_properties_schema)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_verticatable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_verticatable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_webtable(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        webtable_type: str,
        webtable_linked_service_name: "models.LinkedServiceReference",
        webtable_index: object,
        if_match: Optional[str] = None,
        webtable_description: Optional[str] = None,
        webtable_structure: Optional[object] = None,
        webtable_schema: Optional[object] = None,
        webtable_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        webtable_annotations: Optional[List[object]] = None,
        webtable_folder: Optional["models.DatasetFolder"] = None,
        webtable_path: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param webtable_type: Type of dataset.
        :type webtable_type: str
        :param webtable_linked_service_name: Linked service reference.
        :type webtable_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param webtable_index: The zero-based index of the table in the web page. Type: integer (or
         Expression with resultType integer), minimum: 0.
        :type webtable_index: object
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param webtable_description: Dataset description.
        :type webtable_description: str
        :param webtable_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type webtable_structure: object
        :param webtable_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type webtable_schema: object
        :param webtable_parameters: Parameters for dataset.
        :type webtable_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param webtable_annotations: List of tags that can be used for describing the Dataset.
        :type webtable_annotations: list[object]
        :param webtable_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type webtable_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param webtable_path: The relative URL to the web page from the linked service URL. Type:
         string (or Expression with resultType string).
        :type webtable_path: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=webtable_type, description=webtable_description, structure=webtable_structure, schema=webtable_schema, linked_service_name=webtable_linked_service_name, parameters=webtable_parameters, annotations=webtable_annotations, folder=webtable_folder, index=webtable_index, path=webtable_path)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_webtable.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_webtable.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_xeroobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        xeroobject_type: str,
        xeroobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        xeroobject_description: Optional[str] = None,
        xeroobject_structure: Optional[object] = None,
        xeroobject_schema: Optional[object] = None,
        xeroobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        xeroobject_annotations: Optional[List[object]] = None,
        xeroobject_folder: Optional["models.DatasetFolder"] = None,
        xeroobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param xeroobject_type: Type of dataset.
        :type xeroobject_type: str
        :param xeroobject_linked_service_name: Linked service reference.
        :type xeroobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param xeroobject_description: Dataset description.
        :type xeroobject_description: str
        :param xeroobject_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type xeroobject_structure: object
        :param xeroobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type xeroobject_schema: object
        :param xeroobject_parameters: Parameters for dataset.
        :type xeroobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param xeroobject_annotations: List of tags that can be used for describing the Dataset.
        :type xeroobject_annotations: list[object]
        :param xeroobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type xeroobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param xeroobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type xeroobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=xeroobject_type, description=xeroobject_description, structure=xeroobject_structure, schema=xeroobject_schema, linked_service_name=xeroobject_linked_service_name, parameters=xeroobject_parameters, annotations=xeroobject_annotations, folder=xeroobject_folder, table_name=xeroobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_xeroobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_xeroobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}

    async def create_or_update_zohoobject(
        self,
        resource_group_name: str,
        factory_name: str,
        dataset_name: str,
        zohoobject_type: str,
        zohoobject_linked_service_name: "models.LinkedServiceReference",
        if_match: Optional[str] = None,
        zohoobject_description: Optional[str] = None,
        zohoobject_structure: Optional[object] = None,
        zohoobject_schema: Optional[object] = None,
        zohoobject_parameters: Optional[Dict[str, "ParameterSpecification"]] = None,
        zohoobject_annotations: Optional[List[object]] = None,
        zohoobject_folder: Optional["models.DatasetFolder"] = None,
        zohoobject_table_name: Optional[object] = None,
        **kwargs
    ) -> "models.DatasetResource":
        """Creates or updates a dataset.

        :param resource_group_name: The resource group name.
        :type resource_group_name: str
        :param factory_name: The factory name.
        :type factory_name: str
        :param dataset_name: The dataset name.
        :type dataset_name: str
        :param zohoobject_type: Type of dataset.
        :type zohoobject_type: str
        :param zohoobject_linked_service_name: Linked service reference.
        :type zohoobject_linked_service_name: ~azure.mgmt.datafactory.models.LinkedServiceReference
        :param if_match: ETag of the dataset entity.  Should only be specified for update, for which it
         should match existing entity or can be * for unconditional update.
        :type if_match: str
        :param zohoobject_description: Dataset description.
        :type zohoobject_description: str
        :param zohoobject_structure: Columns that define the structure of the dataset. Type: array (or
         Expression with resultType array), itemType: DatasetDataElement.
        :type zohoobject_structure: object
        :param zohoobject_schema: Columns that define the physical type schema of the dataset. Type:
         array (or Expression with resultType array), itemType: DatasetSchemaDataElement.
        :type zohoobject_schema: object
        :param zohoobject_parameters: Parameters for dataset.
        :type zohoobject_parameters: dict[str, ~azure.mgmt.datafactory.models.ParameterSpecification]
        :param zohoobject_annotations: List of tags that can be used for describing the Dataset.
        :type zohoobject_annotations: list[object]
        :param zohoobject_folder: The folder that this Dataset is in. If not specified, Dataset will
         appear at the root level.
        :type zohoobject_folder: ~azure.mgmt.datafactory.models.DatasetFolder
        :param zohoobject_table_name: The table name. Type: string (or Expression with resultType
         string).
        :type zohoobject_table_name: object
        :keyword callable cls: A custom type or function that will be passed the direct response
        :return: DatasetResource or the result of cls(response)
        :rtype: ~azure.mgmt.datafactory.models.DatasetResource
        :raises: ~azure.core.exceptions.HttpResponseError
        """
        cls = kwargs.pop('cls', None)  # type: ClsType["models.DatasetResource"]
        error_map = kwargs.pop('error_map', {404: ResourceNotFoundError, 409: ResourceExistsError})

        _dataset = models.DatasetResource(properties=properties, type=zohoobject_type, description=zohoobject_description, structure=zohoobject_structure, schema=zohoobject_schema, linked_service_name=zohoobject_linked_service_name, parameters=zohoobject_parameters, annotations=zohoobject_annotations, folder=zohoobject_folder, table_name=zohoobject_table_name)
        api_version = "2018-06-01"
        content_type = kwargs.pop("content_type", "application/json")

        # Construct URL
        url = self.create_or_update_zohoobject.metadata['url']
        path_format_arguments = {
            'subscriptionId': self._serialize.url("self._config.subscription_id", self._config.subscription_id, 'str'),
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', max_length=90, min_length=1, pattern=r'^[-\w\._\(\)]+$'),
            'factoryName': self._serialize.url("factory_name", factory_name, 'str', max_length=63, min_length=3, pattern=r'^[A-Za-z0-9]+(?:-[A-Za-z0-9]+)*$'),
            'datasetName': self._serialize.url("dataset_name", dataset_name, 'str', max_length=260, min_length=1, pattern=r'^[A-Za-z0-9_][^<>*#.%&:\\+?/]*$'),
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}  # type: Dict[str, Any]
        query_parameters['api-version'] = self._serialize.query("api_version", api_version, 'str')

        # Construct headers
        header_parameters = {}  # type: Dict[str, Any]
        if if_match is not None:
            header_parameters['If-Match'] = self._serialize.header("if_match", if_match, 'str')
        header_parameters['Content-Type'] = self._serialize.header("content_type", content_type, 'str')
        header_parameters['Accept'] = 'application/json'

        # Construct and send request
        body_content_kwargs = {}  # type: Dict[str, Any]
        body_content = self._serialize.body(_dataset, 'DatasetResource')
        body_content_kwargs['content'] = body_content
        request = self._client.put(url, query_parameters, header_parameters, **body_content_kwargs)

        pipeline_response = await self._client._pipeline.run(request, stream=False, **kwargs)
        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response, error_format=ARMErrorFormat)

        deserialized = self._deserialize('DatasetResource', pipeline_response)

        if cls:
          return cls(pipeline_response, deserialized, {})

        return deserialized
    create_or_update_zohoobject.metadata = {'url': '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataFactory/factories/{factoryName}/datasets/{datasetName}'}
